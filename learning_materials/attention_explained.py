"""
ğŸ¯ Multi-Head Attention è¯¦è§£
==============================

Attention æ˜¯ Transformer çš„çµé­‚ï¼Œè®©æ¨¡å‹èƒ½"å…³æ³¨"è¾“å…¥ä¸­çš„é‡è¦éƒ¨åˆ†ã€‚

æ ¸å¿ƒæ€æƒ³ï¼šå½“æ¨¡å‹å¤„ç†ä¸€ä¸ªè¯æ—¶ï¼Œéœ€è¦å‚è€ƒä¸Šä¸‹æ–‡ä¸­çš„å…¶ä»–è¯
ä¾‹å¦‚ï¼š
  "The animal didn't cross the street because it was too tired."

  å½“å¤„ç† "it" æ—¶ï¼Œæ¨¡å‹åº”è¯¥å…³æ³¨ "animal"ï¼ˆé«˜æ³¨æ„åŠ›æƒé‡ï¼‰
  è€Œä¸æ˜¯ "street"ï¼ˆä½æ³¨æ„åŠ›æƒé‡ï¼‰

MiniMind ä½¿ç”¨çš„æ˜¯ **Grouped Query Attention (GQA)**ï¼š
- Query å¤´ï¼š8 ä¸ªï¼ˆnum_attention_headsï¼‰
- Key/Value å¤´ï¼š2 ä¸ªï¼ˆnum_key_value_headsï¼‰
- æ¯ 4 ä¸ª Q å…±äº« 1 å¯¹ K/Vï¼ˆèŠ‚çœå†…å­˜å’Œè®¡ç®—ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


# ============================================================
# ç®€åŒ–ç‰ˆ Attention ç±»ï¼ˆåŸºäº model_minimind.py:150-222ï¼‰
# ============================================================
class SimplifiedAttention(nn.Module):
    """
    ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå»æ‰äº†ä¸€äº›ç»†èŠ‚ä¾¿äºç†è§£æ ¸å¿ƒé€»è¾‘
    """
    def __init__(self, hidden_size=512, num_heads=8, num_kv_heads=2):
        super().__init__()
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = hidden_size // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦
        self.n_rep = num_heads // num_kv_heads    # KV é‡å¤æ¬¡æ•°

        # ä¸‰ä¸ªæŠ•å½±çŸ©é˜µï¼šQ, K, V
        self.q_proj = nn.Linear(hidden_size, num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)

        # è¾“å‡ºæŠ•å½±
        self.o_proj = nn.Linear(num_heads * self.head_dim, hidden_size, bias=False)

    def forward(self, x):
        """
        x: [batch_size, seq_len, hidden_size]
        """
        batch_size, seq_len, _ = x.shape

        # æ­¥éª¤ 1: æŠ•å½±åˆ° Q, K, V
        # TODO(human)

        print(f"\n" + "="*60)
        print("ğŸ“Š Attention è®¡ç®—æµç¨‹")
        print("="*60)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"Q å½¢çŠ¶: {xq.shape}")
        print(f"K å½¢çŠ¶: {xk.shape}")
        print(f"V å½¢çŠ¶: {xv.shape}")

        # æ­¥éª¤ 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # scores = Q @ K^T / sqrt(head_dim)
        scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
        print(f"\næ³¨æ„åŠ›åˆ†æ•°å½¢çŠ¶: {scores.shape}")
        print(f"åˆ†æ•°èŒƒå›´: [{scores.min().item():.2f}, {scores.max().item():.2f}]")

        # æ­¥éª¤ 3: Causal Maskï¼ˆå› æœæ©ç ï¼Œç¡®ä¿åªèƒ½çœ‹åˆ°ä¹‹å‰çš„è¯ï¼‰
        # åˆ›å»ºä¸Šä¸‰è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä»¥ä¸Šè®¾ä¸º -inf
        causal_mask = torch.triu(
            torch.full((seq_len, seq_len), float("-inf")),
            diagonal=1
        ).to(scores.device)
        scores = scores + causal_mask.unsqueeze(0).unsqueeze(0)

        # æ­¥éª¤ 4: Softmax å½’ä¸€åŒ–ï¼ˆå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼‰
        attn_weights = F.softmax(scores, dim=-1)
        print(f"\næ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
        print(f"æƒé‡ä¹‹å’Œï¼ˆæ¯è¡Œåº”è¯¥=1ï¼‰: {attn_weights[0, 0, 0].sum().item():.4f}")

        # æ­¥éª¤ 5: åŠ æƒæ±‚å’Œ
        output = attn_weights @ xv
        print(f"\nåŠ æƒåè¾“å‡º: {output.shape}")

        # æ­¥éª¤ 6: åˆå¹¶å¤šå¤´ + è¾“å‡ºæŠ•å½±
        output = output.transpose(1, 2).reshape(batch_size, seq_len, -1)
        output = self.o_proj(output)
        print(f"æœ€ç»ˆè¾“å‡º: {output.shape}")

        return output, attn_weights


# ============================================================
# ğŸ§ª å¯è§†åŒ–ç¤ºä¾‹ï¼šè§‚å¯Ÿæ³¨æ„åŠ›æƒé‡
# ============================================================
def visualize_attention_pattern():
    """
    å¯è§†åŒ–ä¸€ä¸ªç®€å•ä¾‹å­çš„æ³¨æ„åŠ›æ¨¡å¼
    """
    print("\n" + "="*60)
    print("ğŸ” æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–")
    print("="*60)

    # åˆ›å»ºä¸€ä¸ªå°çš„æ³¨æ„åŠ›å±‚
    attn = SimplifiedAttention(hidden_size=64, num_heads=4, num_kv_heads=2)
    attn.eval()

    # åˆ›å»ºä¸€ä¸ªçŸ­åºåˆ—
    batch_size = 1
    seq_len = 5
    x = torch.randn(batch_size, seq_len, 64)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output, attn_weights = attn(x)

    # æ‰“å°ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
    print("\nğŸ“‹ ç¬¬ 0 ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    print("   (è¡Œ=æŸ¥è¯¢ä½ç½®, åˆ—=é”®ä½ç½®)")
    print()
    weights = attn_weights[0, 0].cpu().numpy()  # [seq_len, seq_len]

    # æ‰“å°è¡¨å¤´
    print("     ", end="")
    for j in range(seq_len):
        print(f"  K{j}  ", end="")
    print()

    # æ‰“å°æ¯ä¸€è¡Œ
    for i in range(seq_len):
        print(f"  Q{i} ", end="")
        for j in range(seq_len):
            if j > i:  # å› æœæ©ç åŒºåŸŸ
                print("  ---  ", end="")
            else:
                print(f"{weights[i, j]:6.3f}", end=" ")
        print()

    print("\nğŸ’¡ è§‚å¯Ÿ:")
    print("  1. å¯¹è§’çº¿å³ä¸Šæ–¹å…¨æ˜¯ --- (å› æœæ©ç ï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥)")
    print("  2. æ¯è¡Œæƒé‡åŠ èµ·æ¥ = 1.0 (å½’ä¸€åŒ–)")
    print("  3. è¶Šé è¿‘å¯¹è§’çº¿ï¼Œæƒé‡é€šå¸¸è¶Šå¤§ï¼ˆå±€éƒ¨æ€§ï¼‰")


# ============================================================
# ğŸ“š GQA (Grouped Query Attention) è§£é‡Š
# ============================================================
def explain_gqa():
    print("\n" + "="*60)
    print("ğŸ”¬ GQA vs MHA vs MQA å¯¹æ¯”")
    print("="*60)

    print("\n1ï¸âƒ£ **MHA (Multi-Head Attention)** - ä¼ ç»Ÿæ–¹æ¡ˆ")
    print("   Query å¤´: 8")
    print("   Key å¤´:   8")
    print("   Value å¤´: 8")
    print("   â†’ å‚æ•°å¤šï¼Œè®¡ç®—æ…¢ï¼Œä½†æ•ˆæœæœ€å¥½")

    print("\n2ï¸âƒ£ **MQA (Multi-Query Attention)** - æè‡´å‹ç¼©")
    print("   Query å¤´: 8")
    print("   Key å¤´:   1  â† æ‰€æœ‰ Q å…±äº« 1 ä¸ª K")
    print("   Value å¤´: 1  â† æ‰€æœ‰ Q å…±äº« 1 ä¸ª V")
    print("   â†’ å‚æ•°å°‘ï¼Œæ¨ç†å¿«ï¼Œä½†æ•ˆæœç¨å·®")

    print("\n3ï¸âƒ£ **GQA (Grouped Query Attention)** - MiniMind çš„é€‰æ‹©")
    print("   Query å¤´: 8")
    print("   Key å¤´:   2  â† æ¯ 4 ä¸ª Q å…±äº« 1 å¯¹ K/V")
    print("   Value å¤´: 2")
    print("   â†’ å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡ï¼")

    # è®¡ç®—å‚æ•°é‡å¯¹æ¯”
    hidden_size = 512
    head_dim = 64

    mha_params = 3 * 8 * (hidden_size * head_dim)  # Q, K, V å„ 8 å¤´
    mqa_params = 8 * (hidden_size * head_dim) + 2 * (hidden_size * head_dim)  # Q=8, K=1, V=1
    gqa_params = 8 * (hidden_size * head_dim) + 2 * 2 * (hidden_size * head_dim)  # Q=8, K=2, V=2

    print(f"\nğŸ“Š å‚æ•°é‡å¯¹æ¯”ï¼ˆhidden_size={hidden_size}ï¼‰:")
    print(f"   MHA: {mha_params:,} å‚æ•°  (100%)")
    print(f"   MQA: {mqa_params:,} å‚æ•°  ({mqa_params/mha_params*100:.1f}%)")
    print(f"   GQA: {gqa_params:,} å‚æ•°  ({gqa_params/mha_params*100:.1f}%)")
    print(f"   â†’ GQA èŠ‚çœäº† {(1-gqa_params/mha_params)*100:.1f}% çš„ KV å‚æ•°ï¼")


# ============================================================
# ä¸»ç¨‹åº
# ============================================================
if __name__ == "__main__":
    print("="*60)
    print("ğŸ¯ Multi-Head Attention æ·±åº¦è§£æ")
    print("="*60)

    # 1. åŸºç¡€æµç¨‹æ¼”ç¤º
    visualize_attention_pattern()

    # 2. GQA è§£é‡Š
    explain_gqa()

    print("\n" + "="*60)
    print("âœ… æ€»ç»“:")
    print("  - Attention è®©æ¨¡å‹èƒ½\"èšç„¦\"é‡è¦ä¿¡æ¯")
    print("  - å¤šå¤´æœºåˆ¶æ•è·ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»")
    print("  - GQA å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡")
    print("  - Causal mask ç¡®ä¿è‡ªå›å½’ç”Ÿæˆ")
    print("="*60)
