"""
æ•°æ®é›†ç®¡ç†å·¥å…·

æä¾›ç»Ÿä¸€çš„å®éªŒæ•°æ®æ¥å£ï¼Œæ”¯æŒï¼š
- TinyShakespeareï¼ˆç»å…¸å­—ç¬¦çº§æ•°æ®ï¼Œ1MBï¼‰
- TinyStoriesï¼ˆç°ä»£è‹±æ–‡ï¼Œæ”¯æŒå–å­é›†ï¼‰
- åˆæˆæ•°æ®ï¼ˆç”¨äºå¯è§†åŒ–å®éªŒï¼‰

æ³¨æ„ï¼š
    - æ­¤æ–‡ä»¶åŸåä¸º datasets.pyï¼Œå·²é‡å‘½åä¸º data_sources.py
    - é‡å‘½ååŸå› ï¼šé¿å…ä¸ HuggingFace datasets åº“äº§ç”Ÿå‘½åå†²çª
    - è¯¦è§ï¼šhttps://github.com/joyehuang/minimind-notes/pull/20

ç³»ç»Ÿè¦æ±‚ï¼š
    - Python 3.10+ï¼ˆä½¿ç”¨äº†ç±»å‹è”åˆè¯­æ³• str | listï¼‰
    - ä¾èµ–ï¼šrequestsï¼ˆTinyShakespeareï¼‰ã€datasetsï¼ˆTinyStoriesï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from modules.common.data_sources import get_experiment_data

    # è·å– TinyShakespeare
    text = get_experiment_data('shakespeare')

    # è·å– TinyStories å­é›†ï¼ˆ10MBï¼‰
    texts = get_experiment_data('tinystories', size_mb=10)
"""

import os
import requests
from pathlib import Path
from typing import List, Optional
import json

# æ•°æ®ç¼“å­˜ç›®å½•
DATA_DIR = Path(__file__).parent / 'data'
DATA_DIR.mkdir(exist_ok=True)


def get_experiment_data(
    dataset: str = 'shakespeare',
    size_mb: Optional[float] = None,
    cache: bool = True
) -> str | List[str]:
    """
    è·å–å®éªŒæ•°æ®

    Args:
        dataset: æ•°æ®é›†åç§°
            - 'shakespeare': TinyShakespeare (1MB)
            - 'tinystories': TinyStories (å¯æŒ‡å®šå¤§å°)
            - 'synthetic': åˆæˆéšæœºæ•°æ®
        size_mb: æ•°æ®å¤§å°é™åˆ¶ï¼ˆä»…å¯¹ tinystories æœ‰æ•ˆï¼‰
        cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

    Returns:
        str: æ–‡æœ¬æ•°æ®ï¼ˆshakespeare, syntheticï¼‰
        List[str]: æ–‡æœ¬åˆ—è¡¨ï¼ˆtinystoriesï¼‰
    """

    if dataset == 'shakespeare':
        return _get_shakespeare(cache)
    elif dataset == 'tinystories':
        return _get_tinystories(size_mb or 10, cache)
    elif dataset == 'synthetic':
        return _generate_synthetic(size_mb or 1)
    else:
        raise ValueError(f"Unknown dataset: {dataset}")


def _get_shakespeare(cache: bool = True) -> str:
    """ä¸‹è½½ TinyShakespeare æ•°æ®é›†"""

    cache_file = DATA_DIR / 'tinyshakespeare.txt'

    # æ£€æŸ¥ç¼“å­˜
    if cache and cache_file.exists():
        print(f"âœ… ä»ç¼“å­˜åŠ è½½ TinyShakespeare: {cache_file}")
        return cache_file.read_text(encoding='utf-8')

    # ä¸‹è½½
    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
    print(f"ğŸ“¥ ä¸‹è½½ TinyShakespeare from {url}")

    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        text = response.text

        # ä¿å­˜ç¼“å­˜
        if cache:
            cache_file.write_text(text, encoding='utf-8')
            print(f"âœ… å·²ç¼“å­˜åˆ°: {cache_file}")

        print(f"âœ… TinyShakespeare åŠ è½½å®Œæˆ: {len(text):,} å­—ç¬¦")
        return text

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½åˆ°:", cache_file)
        raise


def _get_tinystories(size_mb: float, cache: bool = True) -> List[str]:
    """
    è·å– TinyStories å­é›†

    æ³¨æ„ï¼šéœ€è¦å®‰è£… datasets åº“
        pip install datasets
    """

    cache_file = DATA_DIR / f'tinystories_{size_mb}mb.json'

    # æ£€æŸ¥ç¼“å­˜
    if cache and cache_file.exists():
        print(f"âœ… ä»ç¼“å­˜åŠ è½½ TinyStories: {cache_file}")
        with open(cache_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    # ä¸‹è½½
    try:
        from datasets import load_dataset

        print(f"ğŸ“¥ ä¸‹è½½ TinyStories (ç›®æ ‡å¤§å°: {size_mb} MB)")

        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset('roneneldan/TinyStories', split='train', streaming=True)

        # é€æ­¥åŠ è½½ç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å°
        texts = []
        current_size = 0
        target_size = size_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚

        for example in dataset:
            text = example['text']
            texts.append(text)
            current_size += len(text.encode('utf-8'))

            if current_size >= target_size:
                break

        # ä¿å­˜ç¼“å­˜
        if cache:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(texts, f, ensure_ascii=False)
            print(f"âœ… å·²ç¼“å­˜åˆ°: {cache_file}")

        print(f"âœ… TinyStories åŠ è½½å®Œæˆ: {len(texts):,} ä¸ªæ•…äº‹, {current_size / 1024 / 1024:.2f} MB")
        return texts

    except ImportError:
        print("âŒ ç¼ºå°‘ datasets åº“ï¼Œè¯·å®‰è£…: pip install datasets")
        raise
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        raise


def _generate_synthetic(size_mb: float) -> str:
    """
    ç”Ÿæˆåˆæˆæ•°æ®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰

    ç”Ÿæˆç®€å•çš„é‡å¤æ¨¡å¼ï¼Œç”¨äºéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½å­¦ä¹ 
    """

    # ç®€å•æ¨¡å¼ï¼šé‡å¤å­—æ¯åºåˆ—
    pattern = "abcdefghijklmnopqrstuvwxyz " * 100

    # è®¡ç®—éœ€è¦é‡å¤å¤šå°‘æ¬¡
    target_size = size_mb * 1024 * 1024
    repeats = int(target_size / len(pattern.encode('utf-8'))) + 1

    text = pattern * repeats
    text = text[:int(target_size)]  # ç²¾ç¡®æˆªæ–­

    print(f"âœ… ç”Ÿæˆåˆæˆæ•°æ®: {len(text):,} å­—ç¬¦, {len(text) / 1024 / 1024:.2f} MB")
    return text


def download_all_datasets():
    """ä¸€é”®ä¸‹è½½æ‰€æœ‰æ•°æ®é›†ï¼ˆç”¨äºåˆå§‹åŒ–ï¼‰"""

    print("=" * 60)
    print("å¼€å§‹ä¸‹è½½æ‰€æœ‰å®éªŒæ•°æ®é›†")
    print("=" * 60)

    total_size = 0

    # 1. TinyShakespeare
    print("\n1ï¸âƒ£ TinyShakespeare")
    text = get_experiment_data('shakespeare')
    size = len(text.encode('utf-8'))
    total_size += size
    print(f"   å¤§å°: {size / 1024 / 1024:.2f} MB")

    # 2. TinyStories (10MB å­é›†)
    print("\n2ï¸âƒ£ TinyStories (10MB subset)")
    try:
        texts = get_experiment_data('tinystories', size_mb=10)
        size = sum(len(t.encode('utf-8')) for t in texts)
        total_size += size
        print(f"   å¤§å°: {size / 1024 / 1024:.2f} MB")
    except Exception as e:
        print(f"   âš ï¸ è·³è¿‡ TinyStories: {e}")

    # 3. TinyStories (50MB å­é›†ï¼Œç”¨äºå®Œæ•´è®­ç»ƒ)
    print("\n3ï¸âƒ£ TinyStories (50MB subset)")
    try:
        texts = get_experiment_data('tinystories', size_mb=50)
        size = sum(len(t.encode('utf-8')) for t in texts)
        total_size += size
        print(f"   å¤§å°: {size / 1024 / 1024:.2f} MB")
    except Exception as e:
        print(f"   âš ï¸ è·³è¿‡ TinyStories 50MB: {e}")

    print("\n" + "=" * 60)
    print(f"âœ… ä¸‹è½½å®Œæˆï¼æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
    print(f"ğŸ“‚ æ•°æ®ä½ç½®: {DATA_DIR}")
    print("=" * 60)


if __name__ == '__main__':
    # æµ‹è¯•
    import argparse

    parser = argparse.ArgumentParser(description='æ•°æ®é›†ç®¡ç†å·¥å…·')
    parser.add_argument('--download-all', action='store_true',
                       help='ä¸‹è½½æ‰€æœ‰æ•°æ®é›†')
    parser.add_argument('--dataset', type=str, default='shakespeare',
                       choices=['shakespeare', 'tinystories', 'synthetic'],
                       help='æµ‹è¯•å•ä¸ªæ•°æ®é›†')

    args = parser.parse_args()

    if args.download_all:
        download_all_datasets()
    else:
        data = get_experiment_data(args.dataset)
        if isinstance(data, str):
            print(f"åŠ è½½çš„æ•°æ®: {len(data):,} å­—ç¬¦")
            print(f"å‰ 100 å­—ç¬¦: {data[:100]}")
        else:
            print(f"åŠ è½½çš„æ•°æ®: {len(data):,} ä¸ªæ–‡æœ¬")
            print(f"ç¬¬ä¸€ä¸ªæ–‡æœ¬: {data[0][:100]}")
