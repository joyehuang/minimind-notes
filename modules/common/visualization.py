"""
å¯è§†åŒ–å·¥å…·

æä¾›å¸¸ç”¨çš„å¯è§†åŒ–å‡½æ•°ï¼Œç”¨äºå®éªŒç»“æœå±•ç¤º

åŒ…æ‹¬ï¼š
- æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾
- æ¿€æ´»åˆ†å¸ƒå¯è§†åŒ–
- æ¢¯åº¦æµå¯è§†åŒ–
- å¯¹æ¯”æŸ±çŠ¶å›¾
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Optional


# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜


def plot_attention_heatmap(
    attention_weights: torch.Tensor,
    tokens: Optional[List[str]] = None,
    title: str = "Attention Weights",
    figsize: tuple = (10, 8)
) -> plt.Figure:
    """
    ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾

    Args:
        attention_weights: æ³¨æ„åŠ›æƒé‡ [seq_len, seq_len]
        tokens: Token åˆ—è¡¨ï¼ˆç”¨äºåæ ‡è½´æ ‡ç­¾ï¼‰
        title: å›¾è¡¨æ ‡é¢˜
        figsize: å›¾è¡¨å¤§å°

    Returns:
        fig: matplotlib Figure å¯¹è±¡
    """

    # è½¬æ¢ä¸º numpy
    if isinstance(attention_weights, torch.Tensor):
        attention_weights = attention_weights.detach().cpu().numpy()

    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=figsize)

    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(
        attention_weights,
        cmap='YlOrRd',
        annot=False,
        fmt='.2f',
        cbar=True,
        square=True,
        ax=ax
    )

    # è®¾ç½®æ ‡ç­¾
    if tokens:
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        ax.set_yticklabels(tokens, rotation=0)

    ax.set_xlabel('Key Position')
    ax.set_ylabel('Query Position')
    ax.set_title(title)

    plt.tight_layout()
    return fig


def plot_activation_distribution(
    activations: Dict[str, torch.Tensor],
    title: str = "Activation Distribution",
    figsize: tuple = (12, 6)
) -> plt.Figure:
    """
    ç»˜åˆ¶æ¿€æ´»å€¼åˆ†å¸ƒå¯¹æ¯”

    Args:
        activations: {layer_name: activation_tensor} å­—å…¸
        title: å›¾è¡¨æ ‡é¢˜
        figsize: å›¾è¡¨å¤§å°

    Returns:
        fig: matplotlib Figure å¯¹è±¡
    """

    fig, axes = plt.subplots(1, 2, figsize=figsize)

    layer_names = list(activations.keys())
    colors = plt.cm.viridis(np.linspace(0, 1, len(layer_names)))

    # å·¦å›¾ï¼šåˆ†å¸ƒç›´æ–¹å›¾
    for i, (name, tensor) in enumerate(activations.items()):
        values = tensor.detach().cpu().numpy().flatten()
        axes[0].hist(values, bins=50, alpha=0.5, label=name, color=colors[i])

    axes[0].set_xlabel('Activation Value')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Distribution')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # å³å›¾ï¼šç»Ÿè®¡æŒ‡æ ‡ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ï¼‰
    means = [activations[name].mean().item() for name in layer_names]
    stds = [activations[name].std().item() for name in layer_names]

    x = np.arange(len(layer_names))
    width = 0.35

    axes[1].bar(x - width/2, means, width, label='Mean', color='skyblue')
    axes[1].bar(x + width/2, stds, width, label='Std', color='orange')

    axes[1].set_xlabel('Layer')
    axes[1].set_ylabel('Value')
    axes[1].set_title('Statistics')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(layer_names, rotation=45, ha='right')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3, axis='y')

    plt.suptitle(title)
    plt.tight_layout()
    return fig


def plot_gradient_flow(
    named_parameters,
    title: str = "Gradient Flow",
    figsize: tuple = (12, 6)
) -> plt.Figure:
    """
    ç»˜åˆ¶æ¢¯åº¦æµï¼ˆæ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼‰

    Args:
        named_parameters: model.named_parameters() çš„è¾“å‡º
        title: å›¾è¡¨æ ‡é¢˜
        figsize: å›¾è¡¨å¤§å°

    Returns:
        fig: matplotlib Figure å¯¹è±¡
    """

    ave_grads = []
    max_grads = []
    layers = []

    for name, param in named_parameters:
        if param.requires_grad and param.grad is not None:
            layers.append(name)
            ave_grads.append(param.grad.abs().mean().item())
            max_grads.append(param.grad.abs().max().item())

    fig, ax = plt.subplots(figsize=figsize)

    x = np.arange(len(layers))
    ax.bar(x - 0.2, ave_grads, 0.4, label='Average Gradient', color='skyblue')
    ax.bar(x + 0.2, max_grads, 0.4, label='Max Gradient', color='orange')

    ax.set_xlabel('Layers')
    ax.set_ylabel('Gradient Magnitude')
    ax.set_title(title)
    ax.set_xticks(x)
    ax.set_xticklabels(layers, rotation=90, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_yscale('log')  # å¯¹æ•°åæ ‡ï¼Œä¾¿äºè§‚å¯Ÿ

    plt.tight_layout()
    return fig


def plot_comparison_bars(
    metrics: Dict[str, Dict[str, float]],
    metric_name: str = "Metric",
    title: str = "Comparison",
    figsize: tuple = (10, 6)
) -> plt.Figure:
    """
    ç»˜åˆ¶å¯¹æ¯”æŸ±çŠ¶å›¾

    Args:
        metrics: {config_name: {metric1: value1, metric2: value2}} å­—å…¸
        metric_name: æŒ‡æ ‡åç§°
        title: å›¾è¡¨æ ‡é¢˜
        figsize: å›¾è¡¨å¤§å°

    Returns:
        fig: matplotlib Figure å¯¹è±¡

    Example:
        metrics = {
            'No Norm': {'final_loss': 4.2, 'time': 100},
            'RMSNorm': {'final_loss': 2.1, 'time': 95},
        }
    """

    fig, ax = plt.subplots(figsize=figsize)

    # æå–é…ç½®å’ŒæŒ‡æ ‡
    configs = list(metrics.keys())
    metric_keys = list(next(iter(metrics.values())).keys())

    x = np.arange(len(configs))
    width = 0.8 / len(metric_keys)

    colors = plt.cm.tab10(np.linspace(0, 1, len(metric_keys)))

    # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡
    for i, key in enumerate(metric_keys):
        values = [metrics[config][key] for config in configs]
        offset = (i - len(metric_keys) / 2) * width + width / 2
        ax.bar(x + offset, values, width, label=key, color=colors[i])

    ax.set_xlabel('Configuration')
    ax.set_ylabel(metric_name)
    ax.set_title(title)
    ax.set_xticks(x)
    ax.set_xticklabels(configs, rotation=45, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    return fig


def plot_loss_curves(
    histories: Dict[str, List[float]],
    steps: Optional[List[int]] = None,
    title: str = "Loss Curves",
    figsize: tuple = (10, 6)
) -> plt.Figure:
    """
    ç»˜åˆ¶å¤šæ¡ loss æ›²çº¿å¯¹æ¯”

    Args:
        histories: {config_name: loss_list} å­—å…¸
        steps: æ­¥æ•°åˆ—è¡¨ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ç´¢å¼•ï¼‰
        title: å›¾è¡¨æ ‡é¢˜
        figsize: å›¾è¡¨å¤§å°

    Returns:
        fig: matplotlib Figure å¯¹è±¡
    """

    fig, ax = plt.subplots(figsize=figsize)

    for config_name, losses in histories.items():
        x = steps if steps is not None else list(range(len(losses)))

        # æ£€æŸ¥ NaN
        valid_idx = [i for i, loss in enumerate(losses) if not np.isnan(loss)]

        if len(valid_idx) < len(losses):
            # å‡ºç° NaN
            x_valid = [x[i] for i in valid_idx]
            losses_valid = [losses[i] for i in valid_idx]
            label = f"{config_name} (NaN @ step {x[len(valid_idx)] if len(valid_idx) < len(x) else 'end'})"
            linestyle = '--'
        else:
            x_valid = x
            losses_valid = losses
            label = config_name
            linestyle = '-'

        ax.plot(x_valid, losses_valid, label=label, linestyle=linestyle)

    ax.set_xlabel('Training Steps')
    ax.set_ylabel('Loss')
    ax.set_title(title)
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    return fig


if __name__ == '__main__':
    # æµ‹è¯•
    print("ğŸ¨ æµ‹è¯•å¯è§†åŒ–å·¥å…·")

    # 1. æµ‹è¯•æ³¨æ„åŠ›çƒ­åŠ›å›¾
    attn = torch.softmax(torch.randn(5, 5), dim=-1)
    tokens = ['The', 'cat', 'sat', 'on', 'mat']
    fig = plot_attention_heatmap(attn, tokens, "æµ‹è¯•æ³¨æ„åŠ›çƒ­åŠ›å›¾")
    plt.savefig('modules/common/test_output/test_attention.png')
    plt.close()
    print("âœ… æ³¨æ„åŠ›çƒ­åŠ›å›¾")

    # 2. æµ‹è¯•æ¿€æ´»åˆ†å¸ƒ
    activations = {
        'Layer 1': torch.randn(100, 512),
        'Layer 2': torch.randn(100, 512) * 0.5,
        'Layer 3': torch.randn(100, 512) * 0.1,
    }
    fig = plot_activation_distribution(activations, "æµ‹è¯•æ¿€æ´»åˆ†å¸ƒ")
    plt.savefig('modules/common/test_output/test_activation.png')
    plt.close()
    print("âœ… æ¿€æ´»åˆ†å¸ƒ")

    # 3. æµ‹è¯•å¯¹æ¯”æŸ±çŠ¶å›¾
    metrics = {
        'No Norm': {'final_loss': 4.2, 'time': 100, 'memory': 2.5},
        'LayerNorm': {'final_loss': 2.5, 'time': 95, 'memory': 2.8},
        'RMSNorm': {'final_loss': 2.1, 'time': 90, 'memory': 2.6},
    }
    fig = plot_comparison_bars(metrics, "Value", "æµ‹è¯•å¯¹æ¯”å›¾")
    plt.savefig('modules/common/test_output/test_comparison.png')
    plt.close()
    print("âœ… å¯¹æ¯”æŸ±çŠ¶å›¾")

    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
