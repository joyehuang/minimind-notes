---
title: Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰æ¨¡å— | minimindä»é›¶ç†è§£llmè®­ç»ƒ
description: æ·±å…¥ç†è§£ Self-Attention å¦‚ä½•å·¥ä½œï¼Œä»¥åŠ Qã€Kã€V çš„ç›´è§‰ã€‚é€šè¿‡å¯¹ç…§å®éªŒç†è§£å¤šå¤´æ³¨æ„åŠ›ã€ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„åŸç†å’Œå®ç°ã€‚
keywords: æ³¨æ„åŠ›æœºåˆ¶, Attention, Self-Attention, QKV, å¤šå¤´æ³¨æ„åŠ›, Transformeræ³¨æ„åŠ›, LLMè®­ç»ƒ
---

# 03. Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰

> Self-Attention å¦‚ä½•å·¥ä½œï¼ŸQã€Kã€V çš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Ÿ

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£ Self-Attention çš„æ•°å­¦åŸç†
- âœ… ç†è§£ Qã€Kã€V çš„ç›´è§‰æ„ä¹‰
- âœ… ç†è§£ Multi-Head Attention çš„ä¼˜åŠ¿
- âœ… ç†è§£ GQAï¼ˆGrouped Query Attentionï¼‰
- âœ… ä»é›¶å®ç° Scaled Dot-Product Attention

---

## ğŸ“š å­¦ä¹ è·¯å¾„

### 1ï¸âƒ£ å¿«é€Ÿä½“éªŒï¼ˆ15 åˆ†é’Ÿï¼‰

```bash
cd experiments

# å®éªŒ 1ï¼šAttention åŸºç¡€
python exp1_attention_basics.py

# å®éªŒ 2ï¼šQã€Kã€V è¯¦è§£
python exp2_qkv_explained.py
```

---

## ğŸ”¬ å®éªŒåˆ—è¡¨

| å®éªŒ | ç›®çš„ | æ—¶é—´ |
|------|------|------|
| exp1_attention_basics.py | Attention çš„æ’åˆ—ä¸å˜æ€§å’ŒåŸºç¡€è®¡ç®— | 5åˆ†é’Ÿ |
| exp2_qkv_explained.py | Qã€Kã€V çš„ç›´è§‰ç†è§£ | 5åˆ†é’Ÿ |
| exp3_multihead_attention.py | Multi-Head æœºåˆ¶è¯¦è§£ | 10åˆ†é’Ÿ |

---

## ğŸ’¡ å…³é”®è¦ç‚¹

### 1. Self-Attention çš„æ ¸å¿ƒå…¬å¼

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

**ç›´è§‰**ï¼š
- $QK^T$ï¼šè®¡ç®—"ç›¸å…³æ€§åˆ†æ•°"ï¼ˆè°å’Œè°ç›¸å…³ï¼‰
- $\sqrt{d_k}$ï¼šç¼©æ”¾å› å­ï¼ˆé˜²æ­¢ softmax é¥±å’Œï¼‰
- softmaxï¼šå½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
- $\times V$ï¼šåŠ æƒæ±‚å’Œï¼ˆæå–ç›¸å…³ä¿¡æ¯ï¼‰

---

### 2. Qã€Kã€V çš„ç›´è§‰

| è§’è‰² | å…¨ç§° | é—®é¢˜ | ç±»æ¯” |
|------|------|------|------|
| **Q** | Query | æˆ‘æƒ³æ‰¾ä»€ä¹ˆï¼Ÿ | å›¾ä¹¦é¦†æŸ¥è¯¢ |
| **K** | Key | æˆ‘æä¾›ä»€ä¹ˆæ ‡ç­¾ï¼Ÿ | ä¹¦çš„å…³é”®è¯ |
| **V** | Value | æ‰¾åˆ°åç»™ä»€ä¹ˆï¼Ÿ | ä¹¦çš„å†…å®¹ |

**ä¾‹å­**ï¼šå¤„ç†å¥å­ "The cat sat on the mat"
- "cat" çš„ Queryï¼š"æˆ‘æƒ³æ‰¾ä¸åŠ¨ç‰©ç›¸å…³çš„ä¿¡æ¯"
- "sat" çš„ Keyï¼š"æˆ‘æ˜¯ä¸€ä¸ªåŠ¨ä½œè¯"
- å¦‚æœ QÂ·K åŒ¹é… â†’ å–å‡º "sat" çš„ Value

---

### 3. ä¸ºä»€ä¹ˆéœ€è¦ Multi-Headï¼Ÿ

**é—®é¢˜**ï¼šå•å¤´æ³¨æ„åŠ›åªèƒ½å­¦ä¹ ä¸€ç§"å…³ç³»æ¨¡å¼"

**è§£å†³**ï¼šå¤šå¤´å¹¶è¡Œï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒæ¨¡å¼
- Head 1ï¼šè¯­æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰
- Head 2ï¼šè¯­ä¹‰å…³ç³»ï¼ˆåŒä¹‰è¯ï¼‰
- Head 3ï¼šä½ç½®å…³ç³»ï¼ˆç›¸é‚»è¯ï¼‰
- ...

**å…¬å¼**ï¼š
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$

---

### 4. GQAï¼ˆGrouped Query Attentionï¼‰

**MHA**ï¼ˆMulti-Head Attentionï¼‰ï¼šæ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„ Qã€Kã€V
- å‚æ•°ï¼š3 Ã— n_heads Ã— head_dim

**GQA**ï¼šå¤šä¸ª Q å¤´å…±äº«ä¸€ç»„ Kã€V
- å‚æ•°ï¼š(n_heads + 2 Ã— n_kv_heads) Ã— head_dim
- å‡å°‘ KV Cacheï¼ŒåŠ é€Ÿæ¨ç†

**MiniMind**ï¼š`n_heads=8, n_kv_heads=2`
- 8 ä¸ª Q å¤´ï¼Œ2 ä¸ª KV å¤´
- æ¯ 4 ä¸ª Q å¤´å…±äº«ä¸€ç»„ KV

---

## ğŸ“– æ–‡æ¡£

- ğŸ“˜ [teaching.md](./teaching.md) - å®Œæ•´çš„æ¦‚å¿µè®²è§£
- ğŸ’» [code_guide.md](./code_guide.md) - MiniMind æºç å¯¼è¯»
- ğŸ“ [quiz.md](./quiz.md) - è‡ªæµ‹é¢˜

---

## âœ… å®Œæˆæ£€æŸ¥

å­¦å®Œæœ¬æ¨¡å—åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

### ç†è®º
- [ ] å†™å‡º Attention çš„æ•°å­¦å…¬å¼
- [ ] è§£é‡Š Qã€Kã€V çš„ä½œç”¨
- [ ] è§£é‡Šç¼©æ”¾å› å­ $\sqrt{d_k}$ çš„ä½œç”¨
- [ ] è§£é‡Š Multi-Head çš„ä¼˜åŠ¿

### å®è·µ
- [ ] ä»é›¶å®ç° Scaled Dot-Product Attention
- [ ] å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
- [ ] ç†è§£å› æœæ©ç ï¼ˆCausal Maskï¼‰

---

## ğŸ”— ç›¸å…³èµ„æº

### è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/abs/2305.13245)

### ä»£ç å®ç°
- MiniMind: `model/model_minimind.py:250-330`

---

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ¨¡å—åï¼Œå‰å¾€ï¼š
ğŸ‘‰ [04. FeedForwardï¼ˆå‰é¦ˆç½‘ç»œï¼‰](../04-feedforward)
