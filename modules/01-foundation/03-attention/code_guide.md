# Attention ä»£ç å¯¼è¯»

> ç†è§£ MiniMind ä¸­ Attention çš„çœŸå®å®ç°

---

## ğŸ“‚ ä»£ç ä½ç½®

### 1. Attention ç±»

**æ–‡ä»¶**ï¼š`model/model_minimind.py`
**è¡Œæ•°**ï¼š250-330

```python
class Attention(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.n_heads = config.num_attention_heads       # 8
        self.n_kv_heads = config.num_key_value_heads   # 2 (GQA)
        self.head_dim = config.hidden_size // self.n_heads  # 64
        self.n_rep = self.n_heads // self.n_kv_heads   # 4

        # QKV æŠ•å½±
        self.wq = nn.Linear(config.hidden_size, self.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(config.hidden_size, self.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(config.hidden_size, self.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(self.n_heads * self.head_dim, config.hidden_size, bias=False)

    def forward(self, x, pos_ids, mask):
        batch, seq_len, _ = x.shape

        # 1. è®¡ç®— Q, K, V
        xq = self.wq(x).view(batch, seq_len, self.n_heads, self.head_dim)
        xk = self.wk(x).view(batch, seq_len, self.n_kv_heads, self.head_dim)
        xv = self.wv(x).view(batch, seq_len, self.n_kv_heads, self.head_dim)

        # 2. åº”ç”¨ RoPE
        xq, xk = apply_rotary_emb(xq, xk, self.freqs_cis[pos_ids])

        # 3. GQAï¼šæ‰©å±• KV ä»¥åŒ¹é… Q çš„å¤´æ•°
        xk = repeat_kv(xk, self.n_rep)  # [batch, seq, n_heads, head_dim]
        xv = repeat_kv(xv, self.n_rep)

        # 4. è½¬ç½®ä»¥ä¾¿çŸ©é˜µä¹˜æ³•
        xq = xq.transpose(1, 2)  # [batch, n_heads, seq, head_dim]
        xk = xk.transpose(1, 2)
        xv = xv.transpose(1, 2)

        # 5. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # 6. åº”ç”¨å› æœæ©ç 
        if mask is not None:
            scores = scores + mask

        # 7. Softmax
        attn_weights = F.softmax(scores, dim=-1)

        # 8. åŠ æƒæ±‚å’Œ
        output = torch.matmul(attn_weights, xv)

        # 9. åˆå¹¶å¤´ + è¾“å‡ºæŠ•å½±
        output = output.transpose(1, 2).contiguous().view(batch, seq_len, -1)
        return self.wo(output)
```

---

### 2. GQAï¼šrepeat_kv å‡½æ•°

```python
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """å°† KV å¤´æ‰©å±•ä»¥åŒ¹é… Q å¤´æ•°"""
    if n_rep == 1:
        return x

    batch, seq_len, n_kv_heads, head_dim = x.shape

    # [batch, seq, n_kv_heads, 1, head_dim]
    x = x[:, :, :, None, :]

    # æ‰©å±•å¹¶é‡å¡‘
    x = x.expand(batch, seq_len, n_kv_heads, n_rep, head_dim)
    return x.reshape(batch, seq_len, n_kv_heads * n_rep, head_dim)
```

**æ•ˆæœ**ï¼š
- è¾“å…¥ï¼š`[batch, seq, 2, 64]`ï¼ˆ2 ä¸ª KV å¤´ï¼‰
- n_rep = 4
- è¾“å‡ºï¼š`[batch, seq, 8, 64]`ï¼ˆ8 ä¸ªå¤´ï¼ŒåŒ¹é… Qï¼‰

---

## ğŸ” å…³é”®å®ç°ç»†èŠ‚

### 1. ç¼©æ”¾å› å­

```python
scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
```

**ä¸ºä»€ä¹ˆé™¤ä»¥ $\sqrt{d_k}$ï¼Ÿ**
- ç‚¹ç§¯çš„æœŸæœ›æ–¹å·® = d_k
- å¤§æ–¹å·® â†’ softmax æ¢¯åº¦æ¶ˆå¤±
- é™¤ä»¥ $\sqrt{d_k}$ ä½¿æ–¹å·® = 1

---

### 2. å› æœæ©ç 

```python
if mask is not None:
    scores = scores + mask
```

**mask çš„å€¼**ï¼š
- 0ï¼šå…è®¸æ³¨æ„
- $-\infty$ï¼šç¦æ­¢æ³¨æ„ï¼ˆsoftmax å = 0ï¼‰

**ç”Ÿæˆæ–¹å¼**ï¼š
```python
mask = torch.triu(torch.full((seq_len, seq_len), float('-inf')), diagonal=1)
```

---

### 3. Flash Attentionï¼ˆå¯é€‰ï¼‰

```python
if self.flash_attn:
    output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=mask)
else:
    # æ‰‹åŠ¨å®ç°
    scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
    ...
```

**Flash Attention ä¼˜åŠ¿**ï¼š
- å†…å­˜æ•ˆç‡æ›´é«˜ï¼ˆä¸å­˜å‚¨å®Œæ•´ attention çŸ©é˜µï¼‰
- é€Ÿåº¦æ›´å¿«ï¼ˆèåˆæ“ä½œï¼‰

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. å½¢çŠ¶å˜æ¢é¡ºåº

```python
# è¾“å…¥ï¼š[batch, seq, hidden]
xq = self.wq(x)                    # [batch, seq, n_heads * head_dim]
xq = xq.view(batch, seq, n_heads, head_dim)  # åˆ†å¤´
xq = xq.transpose(1, 2)            # [batch, n_heads, seq, head_dim]
```

**ä¸ºä»€ä¹ˆè¦ transposeï¼Ÿ**
- çŸ©é˜µä¹˜æ³•éœ€è¦ `[..., seq, dim] @ [..., dim, seq]`
- transpose åå½¢çŠ¶åŒ¹é…

---

### 2. contiguous() çš„å¿…è¦æ€§

```python
output = output.transpose(1, 2).contiguous().view(batch, seq_len, -1)
```

**ä¸ºä»€ä¹ˆéœ€è¦ contiguousï¼Ÿ**
- `transpose` ä¸æ”¹å˜å†…å­˜å¸ƒå±€ï¼Œåªæ”¹å˜è§†å›¾
- `view` éœ€è¦è¿ç»­å†…å­˜
- `contiguous()` é‡æ–°æ’åˆ—å†…å­˜

---

## ğŸ¯ åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

```python
# ä¿å­˜ attention weights
attn_weights = F.softmax(scores, dim=-1)
# ç»˜åˆ¶çƒ­åŠ›å›¾
plt.imshow(attn_weights[0, 0].detach().numpy())
```

### ç»ƒä¹  2ï¼šç§»é™¤ç¼©æ”¾å› å­

ä¿®æ”¹ä»£ç ï¼Œç§»é™¤ `/math.sqrt(self.head_dim)`ï¼Œè§‚å¯Ÿ softmax è¾“å‡ºçš„å˜åŒ–ã€‚

### ç»ƒä¹  3ï¼šå®ç° KV Cache

åœ¨æ¨ç†æ—¶ç¼“å­˜ä¹‹å‰çš„ Kã€Vï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- MiniMind å®Œæ•´ä»£ç ï¼š`model/model_minimind.py`
- Flash Attention è®ºæ–‡ï¼š[arXiv:2205.14135](https://arxiv.org/abs/2205.14135)
- PyTorch SDPAï¼š[scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
