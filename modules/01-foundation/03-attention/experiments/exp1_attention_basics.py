"""
ğŸ¯ Attention æœºåˆ¶ä»é›¶ç†è§£
===========================

ç”¨æœ€ç®€å•çš„ä¾‹å­å¸®ä½ ç†è§£ï¼š
1. Qã€Kã€V æ˜¯ä»€ä¹ˆ
2. Attention å¦‚ä½•è®¡ç®—
3. ä¸ºä»€ä¹ˆéœ€è¦ Attention
"""

import torch
import torch.nn.functional as F


# ============================================================
# å®éªŒ 1: ç†è§£ Qã€Kã€V çš„æ¦‚å¿µ
# ============================================================
def explain_qkv_concept():
    print("="*70)
    print("ğŸ“š å®éªŒ 1: ç†è§£ Queryã€Keyã€Value")
    print("="*70)

    print("""
ğŸ” ç±»æ¯”ï¼šåœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦

åœºæ™¯ï¼šä½ æƒ³æ‰¾ä¸€æœ¬å…³äº"æœºå™¨å­¦ä¹ "çš„ä¹¦

1. Query (æŸ¥è¯¢):
   - ä½ çš„éœ€æ±‚ï¼š"æˆ‘æƒ³æ‰¾æœºå™¨å­¦ä¹ çš„ä¹¦"
   - åœ¨ Attention ä¸­ï¼šå½“å‰è¯æƒ³è¦ä»€ä¹ˆä¿¡æ¯

2. Key (é”®):
   - æ¯æœ¬ä¹¦çš„æ ‡ç­¾ï¼š"Pythonç¼–ç¨‹"ã€"æœºå™¨å­¦ä¹ "ã€"å°è¯´"...
   - åœ¨ Attention ä¸­ï¼šæ¯ä¸ªè¯æä¾›çš„ä¿¡æ¯ç±»å‹

3. Value (å€¼):
   - ä¹¦çš„å®é™…å†…å®¹
   - åœ¨ Attention ä¸­ï¼šæ¯ä¸ªè¯çš„å…·ä½“ä¿¡æ¯

åŒ¹é…è¿‡ç¨‹:
   ä½ çš„æŸ¥è¯¢ vs æ¯æœ¬ä¹¦çš„æ ‡ç­¾ â†’ è®¡ç®—ç›¸ä¼¼åº¦
   ç›¸ä¼¼åº¦é«˜çš„ä¹¦ â†’ ä»”ç»†é˜…è¯»ï¼ˆé«˜æƒé‡ï¼‰
   ç›¸ä¼¼åº¦ä½çš„ä¹¦ â†’ å¿½ç•¥ï¼ˆä½æƒé‡ï¼‰
    """)


# ============================================================
# å®éªŒ 2: æœ€ç®€å•çš„ Attention è®¡ç®—
# ============================================================
def simple_attention_example():
    print("\n" + "="*70)
    print("ğŸ§® å®éªŒ 2: æœ€ç®€å•çš„ Attention è®¡ç®—")
    print("="*70)

    # å‡è®¾æœ‰3ä¸ªè¯çš„å¥å­ï¼š"I love cats"
    # ç”¨ç®€åŒ–çš„3ç»´å‘é‡è¡¨ç¤º
    print("\nå¥å­: \"I love cats\"")
    print("ç”¨ç®€åŒ–çš„3ç»´å‘é‡è¡¨ç¤ºæ¯ä¸ªè¯:\n")

    # è¯å‘é‡ï¼ˆä¸ºäº†æ¼”ç¤ºï¼Œæ‰‹åŠ¨è®¾ç½®ç®€å•çš„å€¼ï¼‰
    words = torch.tensor([
        [1.0, 0.0, 0.0],  # "I"
        [0.0, 1.0, 0.0],  # "love"
        [0.0, 0.0, 1.0],  # "cats"
    ])

    print("è¯å‘é‡:")
    print("  I:    [1.0, 0.0, 0.0]")
    print("  love: [0.0, 1.0, 0.0]")
    print("  cats: [0.0, 0.0, 1.0]")

    # ä¸ºäº†ç®€åŒ–ï¼Œè®© Q = K = V = wordsï¼ˆå®é™…ä¸­æ˜¯ä¸åŒçš„æŠ•å½±ï¼‰
    Q = words  # Query: æ¯ä¸ªè¯æƒ³è¦ä»€ä¹ˆä¿¡æ¯
    K = words  # Key: æ¯ä¸ªè¯æä¾›ä»€ä¹ˆä¿¡æ¯
    V = words  # Value: æ¯ä¸ªè¯çš„å®é™…å†…å®¹

    print("\n" + "-"*70)
    print("æ­¥éª¤ 1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Q @ K^T)")
    print("-"*70)

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = Q @ K.T  # [3, 3]

    print("\næ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ:")
    print("       I    love  cats")
    for i, word in enumerate(["I", "love", "cats"]):
        print(f"  {word:4s} ", end="")
        for j in range(3):
            print(f"{scores[i, j].item():5.1f} ", end="")
        print()

    print("\nğŸ’¡ ç†è§£:")
    print("  - å¯¹è§’çº¿éƒ½æ˜¯ 1.0: æ¯ä¸ªè¯å’Œè‡ªå·±æœ€ç›¸ä¼¼")
    print("  - å…¶ä»–ä½ç½®éƒ½æ˜¯ 0.0: è¿™äº›è¯å‘é‡äº’ç›¸æ­£äº¤ï¼ˆç‰¹æ„è®¾è®¡çš„ï¼‰")

    print("\n" + "-"*70)
    print("æ­¥éª¤ 2: Softmax å½’ä¸€åŒ–ï¼ˆå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼‰")
    print("-"*70)

    # Softmax
    attention_weights = F.softmax(scores, dim=-1)

    print("\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    print("       I     love   cats")
    for i, word in enumerate(["I", "love", "cats"]):
        print(f"  {word:4s} ", end="")
        for j in range(3):
            print(f"{attention_weights[i, j].item():.3f} ", end="")
        print(f"  (å’Œ={attention_weights[i].sum().item():.3f})")

    print("\nğŸ’¡ ç†è§£:")
    print("  - æ¯è¡Œçš„æƒé‡åŠ èµ·æ¥ = 1.0")
    print("  - æƒé‡è¡¨ç¤º'å…³æ³¨ç¨‹åº¦'")

    print("\n" + "-"*70)
    print("æ­¥éª¤ 3: åŠ æƒæ±‚å’Œ (Attention Ã— V)")
    print("-"*70)

    # åŠ æƒæ±‚å’Œ
    output = attention_weights @ V

    print("\nè¾“å‡ºå‘é‡:")
    for i, word in enumerate(["I", "love", "cats"]):
        print(f"  {word}: {output[i].numpy()}")

    print("\nğŸ’¡ ç†è§£:")
    print("  - æ¯ä¸ªè¯çš„è¾“å‡ºæ˜¯æ‰€æœ‰è¯ Value çš„åŠ æƒå¹³å‡")
    print("  - æƒé‡ç”±æ³¨æ„åŠ›åˆ†æ•°å†³å®š")


# ============================================================
# å®éªŒ 3: çœŸå®åœºæ™¯ - "it" æŒ‡ä»£æ¶ˆæ­§
# ============================================================
def realistic_attention_example():
    print("\n\n" + "="*70)
    print("ğŸŒŸ å®éªŒ 3: çœŸå®åœºæ™¯ - ä»£è¯æ¶ˆæ­§")
    print("="*70)

    print("""
å¥å­: "The animal didn't cross the street because it was too tired."

é—®é¢˜: "it" æŒ‡ä»£ä»€ä¹ˆï¼Ÿ
  é€‰é¡¹ A: animal  â† æ­£ç¡®ï¼
  é€‰é¡¹ B: street

Attention å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ
    """)

    # ç®€åŒ–ç¤ºä¾‹ï¼šåªå…³æ³¨ "animal", "street", "it" ä¸‰ä¸ªè¯
    # ç”¨ 5 ç»´å‘é‡è¡¨ç¤ºï¼ˆå®é™…ä¸­æ˜¯ 512 ç»´ï¼‰

    print("ç®€åŒ–è¡¨ç¤ºï¼ˆåªçœ‹å…³é”®è¯ï¼‰:")
    print()

    # æ‰‹å·¥è®¾è®¡çš„å‘é‡ï¼ˆæ¨¡æ‹ŸçœŸå®çš„è¯åµŒå…¥ï¼‰
    # "animal" å’Œ "it" çš„å‘é‡æ¯”è¾ƒç›¸ä¼¼
    animal = torch.tensor([0.8, 0.6, 0.2, 0.1, 0.9])
    street = torch.tensor([0.2, 0.1, 0.9, 0.8, 0.1])
    it = torch.tensor([0.7, 0.5, 0.3, 0.2, 0.8])  # å’Œ animal æ›´ç›¸ä¼¼

    # ç»„åˆæˆçŸ©é˜µ
    words = torch.stack([animal, street, it])
    word_names = ["animal", "street", "it"]

    print("è¯å‘é‡ï¼ˆç®€åŒ–çš„5ç»´ï¼‰:")
    for name, vec in zip(word_names, words):
        print(f"  {name:6s}: {vec.numpy()}")

    # TODO(human)

    print("\n" + "="*70)
    print("è§‚å¯Ÿ:")
    print("="*70)
    print(f"  - it å¯¹ animal çš„æ³¨æ„åŠ›: {attention_weights[2, 0].item():.3f}")
    print(f"  - it å¯¹ street çš„æ³¨æ„åŠ›: {attention_weights[2, 1].item():.3f}")
    print(f"  - it å¯¹ è‡ªå·± çš„æ³¨æ„åŠ›:   {attention_weights[2, 2].item():.3f}")

    print(f"\nâœ… 'it' æœ€å…³æ³¨ '{word_names[attention_weights[2].argmax().item()]}'!")
    print("   è¿™å°±æ˜¯ Attention å¦‚ä½•å¸®åŠ©æ¨¡å‹ç†è§£ä»£è¯æŒ‡ä»£çš„ï¼")


# ============================================================
# æ€»ç»“
# ============================================================
def summary():
    print("\n\n" + "="*70)
    print("ğŸ“š æ€»ç»“ï¼šAttention æœºåˆ¶çš„æœ¬è´¨")
    print("="*70)

    print("""
1ï¸âƒ£  ä¸‰ä¸ªè§’è‰²:
    Query (Q): æˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
    Key   (K): æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
    Value (V): å…·ä½“ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ

2ï¸âƒ£  è®¡ç®—æµç¨‹:
    â‘  ç›¸ä¼¼åº¦: Q @ K^T  (æŸ¥è¯¢å’Œæ¯ä¸ªé”®çš„ç›¸ä¼¼åº¦)
    â‘¡ å½’ä¸€åŒ–: Softmax   (è½¬æ¢ä¸ºæƒé‡)
    â‘¢ åŠ æƒå’Œ: Attention @ V  (æŒ‰æƒé‡ç»„åˆä¿¡æ¯)

3ï¸âƒ£  æ ¸å¿ƒæ€æƒ³:
    - è®©æ¨¡å‹èƒ½"æŸ¥çœ‹"æ‰€æœ‰ä¸Šä¸‹æ–‡
    - è‡ªåŠ¨å­¦ä¼šå…³æ³¨é‡è¦çš„ä¿¡æ¯
    - å¿½ç•¥ä¸ç›¸å…³çš„ä¿¡æ¯

4ï¸âƒ£  ä¸ºä»€ä¹ˆå« "Self-Attention"ï¼Ÿ
    - "Self" = è‡ªå·±çœ‹è‡ªå·±
    - å¥å­ä¸­çš„è¯äº’ç›¸çœ‹ï¼ˆè€Œä¸æ˜¯çœ‹å¤–éƒ¨ä¿¡æ¯ï¼‰
    - Qã€Kã€V éƒ½æ¥è‡ªåŒä¸€ä¸ªå¥å­

5ï¸âƒ£  åœ¨ MiniMind ä¸­:
    - æ¯ä¸ª Transformer Block éƒ½æœ‰ä¸€ä¸ª Attention å±‚
    - ä½¿ç”¨ 8 ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆMulti-Head Attentionï¼‰
    - ä½¿ç”¨ GQAï¼ˆGrouped Query Attentionï¼‰ä¼˜åŒ–
    """)

    print("="*70)


# ============================================================
# è¿è¡Œæ‰€æœ‰å®éªŒ
# ============================================================
if __name__ == "__main__":
    print("\nğŸ¯ Attention æœºåˆ¶ä»é›¶ç†è§£\n")

    # å®éªŒ 1: æ¦‚å¿µè§£é‡Š
    explain_qkv_concept()

    # å®éªŒ 2: æœ€ç®€å•çš„è®¡ç®—
    simple_attention_example()

    # å®éªŒ 3: çœŸå®åœºæ™¯
    realistic_attention_example()

    # æ€»ç»“
    summary()

    print("\nğŸ’­ æ€è€ƒé¢˜:")
    print("="*70)
    print("""
1. ä¸ºä»€ä¹ˆéœ€è¦ Softmaxï¼Ÿ
   â†’ ç¡®ä¿æƒé‡åŠ èµ·æ¥=1ï¼Œå˜æˆæ¦‚ç‡åˆ†å¸ƒ

2. å¦‚æœå»æ‰ Attentionï¼Œåªç”¨ FeedForward è¡Œå—ï¼Ÿ
   â†’ ä¸è¡Œï¼æ¨¡å‹æ— æ³•çœ‹åˆ°ä¸Šä¸‹æ–‡ï¼Œåªèƒ½é€è¯å¤„ç†

3. Qã€Kã€V ä¸ºä»€ä¹ˆè¦åˆ†å¼€ï¼Ÿ
   â†’ çµæ´»æ€§ï¼å¯ä»¥å­¦ä¹ "æˆ‘æƒ³è¦ä»€ä¹ˆ"å’Œ"æˆ‘æœ‰ä»€ä¹ˆ"çš„ä¸åŒè¡¨ç¤º

4. Attention åˆ†æ•°å¾ˆå¤§ä¼šæ€æ ·ï¼Ÿ
   â†’ Softmax åä¼šå˜æˆæ¥è¿‘ [0, 0, ..., 1]ï¼ˆåªå…³æ³¨ä¸€ä¸ªè¯ï¼‰
      æ‰€ä»¥è¦é™¤ä»¥ sqrt(head_dim) æ¥ç¼©æ”¾
    """)
    print("="*70)
