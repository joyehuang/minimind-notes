# FeedForward ä»£ç å¯¼è¯»

> ç†è§£ MiniMind ä¸­ FeedForward çš„çœŸå®å®ç°

---

## ğŸ“‚ ä»£ç ä½ç½®

### 1. FeedForward ç±»

**æ–‡ä»¶**ï¼š`model/model_minimind.py`
**è¡Œæ•°**ï¼š330-380

```python
class FeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()

        hidden_dim = config.hidden_size
        intermediate_dim = config.intermediate_size

        # SwiGLU: ä¸‰ä¸ªæŠ•å½±çŸ©é˜µ
        self.gate_proj = nn.Linear(hidden_dim, intermediate_dim, bias=False)
        self.up_proj = nn.Linear(hidden_dim, intermediate_dim, bias=False)
        self.down_proj = nn.Linear(intermediate_dim, hidden_dim, bias=False)

    def forward(self, x):
        # SwiGLU å…¬å¼
        # output = down(SiLU(gate(x)) * up(x))
        return self.down_proj(
            F.silu(self.gate_proj(x)) * self.up_proj(x)
        )
```

---

### 2. åœ¨ TransformerBlock ä¸­çš„ä½¿ç”¨

**æ–‡ä»¶**ï¼š`model/model_minimind.py`
**è¡Œæ•°**ï¼š400-450

```python
class TransformerBlock(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.attention = Attention(config)
        self.feed_forward = FeedForward(config)
        self.attention_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(self, x, pos_ids, mask):
        # Attention + æ®‹å·®
        h = x + self.attention(self.attention_norm(x), pos_ids, mask)

        # FeedForward + æ®‹å·®
        out = h + self.feed_forward(self.ffn_norm(h))

        return out
```

---

## ğŸ” é€æ­¥è§£æ

### SwiGLU çš„ä¸‰ä¸ªæŠ•å½±

```python
# è¾“å…¥ x: [batch, seq, hidden_dim]

# 1. è®¡ç®—é—¨æ§ä¿¡å·
gate = self.gate_proj(x)  # [batch, seq, intermediate_dim]

# 2. è®¡ç®—å€¼ä¿¡å·
up = self.up_proj(x)      # [batch, seq, intermediate_dim]

# 3. SiLU æ¿€æ´» + é—¨æ§
hidden = F.silu(gate) * up  # [batch, seq, intermediate_dim]

# 4. å‹ç¼©å›åŸç»´åº¦
output = self.down_proj(hidden)  # [batch, seq, hidden_dim]
```

**ç»´åº¦å˜åŒ–**ï¼ˆMiniMind 512 é…ç½®ï¼‰ï¼š
```
è¾“å…¥:  [batch, seq, 512]
gate:  [batch, seq, 2048]  (æ‰©å¼ )
up:    [batch, seq, 2048]  (æ‰©å¼ )
hidden: [batch, seq, 2048]  (gate Ã— up)
è¾“å‡º:  [batch, seq, 512]   (å‹ç¼©)
```

---

### SiLU æ¿€æ´»å‡½æ•°

```python
# F.silu(x) = x * torch.sigmoid(x)

x = torch.tensor([-2, -1, 0, 1, 2], dtype=torch.float)
silu = F.silu(x)
# tensor([-0.2384, -0.2689,  0.0000,  0.7311,  1.7616])

# å¯¹æ¯” ReLU
relu = F.relu(x)
# tensor([0., 0., 0., 1., 2.])
```

**ç‰¹ç‚¹**ï¼š
- å¹³æ»‘ï¼šå¤„å¤„å¯å¯¼ï¼Œæ¢¯åº¦ç¨³å®š
- éå•è°ƒï¼šè´Ÿæ•°éƒ¨åˆ†ä¸å®Œå…¨ä¸º 0
- è‡ªé—¨æ§ï¼š$x \cdot \sigma(x)$

---

### ä¸ºä»€ä¹ˆç”¨ä¸‰ä¸ªæŠ•å½±è€Œä¸æ˜¯ä¸¤ä¸ªï¼Ÿ

**æ ‡å‡† FFNï¼ˆä¸¤ä¸ªæŠ•å½±ï¼‰**ï¼š
```python
hidden = ReLU(W1(x))  # 768 â†’ 2048
output = W2(hidden)   # 2048 â†’ 768
```

**SwiGLUï¼ˆä¸‰ä¸ªæŠ•å½±ï¼‰**ï¼š
```python
gate = SiLU(W_gate(x))  # 768 â†’ 2048
up = W_up(x)            # 768 â†’ 2048
hidden = gate * up      # é€å…ƒç´ ç›¸ä¹˜
output = W_down(hidden) # 2048 â†’ 768
```

**ä¼˜åŠ¿**ï¼š
1. é—¨æ§æœºåˆ¶ï¼šåŠ¨æ€æ§åˆ¶ä¿¡æ¯æµ
2. æ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼šä¸¤æ¡è·¯å¾„æä¾›ä¸åŒè§†è§’
3. å®éªŒæ•ˆæœæ›´å¥½ï¼šåœ¨å„ç§ LLM åŸºå‡†ä¸Šè¡¨ç°æ›´ä¼˜

**å‚æ•°é‡å¯¹æ¯”**ï¼š
- æ ‡å‡† FFNï¼š2 Ã— d Ã— 4d = 8dÂ²
- SwiGLUï¼š3 Ã— d Ã— (8d/3) = 8dÂ²ï¼ˆè°ƒæ•´ intermediateï¼‰

---

### é—¨æ§æœºåˆ¶è¯¦è§£

```python
gate = F.silu(self.gate_proj(x))  # é—¨æ§ä¿¡å·ï¼šå†³å®š"å¼€å…³ç¨‹åº¦"
up = self.up_proj(x)              # å€¼ä¿¡å·ï¼šå®é™…å†…å®¹
hidden = gate * up                # é€å…ƒç´ ç›¸ä¹˜

# gate çš„ä½œç”¨ï¼š
# - gate â‰ˆ 0ï¼šå…³é—­ï¼Œup çš„ä¿¡æ¯è¢«æŠ‘åˆ¶
# - gate â‰ˆ 1ï¼šæ‰“å¼€ï¼Œup çš„ä¿¡æ¯å®Œå…¨é€šè¿‡
# - 0 < gate < 1ï¼šéƒ¨åˆ†é€šè¿‡
```

**ç›´è§‰**ï¼š
- gate åƒä¸€ä¸ª"éŸ³é‡æ—‹é’®"
- ä¸åŒç»´åº¦æœ‰ä¸åŒçš„"éŸ³é‡"
- æ¨¡å‹å­¦ä¹ å“ªäº›ä¿¡æ¯åº”è¯¥æ”¾å¤§/æŠ‘åˆ¶

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. æ— åç½®ï¼ˆbias=Falseï¼‰

```python
self.gate_proj = nn.Linear(hidden_dim, intermediate_dim, bias=False)
```

**ä¸ºä»€ä¹ˆä¸ç”¨åç½®ï¼Ÿ**
- å¤§æ¨¡å‹ä¸­åç½®æ•ˆæœä¸æ˜æ˜¾
- å‡å°‘å‚æ•°é‡
- ä¸ RMSNorm é…åˆæ›´å¥½ï¼ˆå·²ç»æœ‰å½’ä¸€åŒ–ï¼‰

---

### 2. intermediate_size çš„é€‰æ‹©

```python
# MiniMind é…ç½®
hidden_size = 512
intermediate_size = 2048  # 4x æ‰©å¼ 

# å¦‚æœç”¨ SwiGLUï¼Œæœ‰äº›å®ç°ä¼šè°ƒæ•´ï¼š
# intermediate_size = int(hidden_size * 4 * 2 / 3)
# ä»¥ä¿æŒæ€»å‚æ•°é‡ä¸æ ‡å‡† FFN ç›¸åŒ
```

**Llama çš„åšæ³•**ï¼š
- intermediate_size = 2.7 Ã— hidden_sizeï¼ˆè°ƒæ•´åï¼‰
- æˆ–ç›´æ¥ç”¨ 4x ä½†æ¥å—æ›´å¤šå‚æ•°

---

### 3. èåˆæ“ä½œ

```python
# æœ´ç´ å®ç°
gate = self.gate_proj(x)
up = self.up_proj(x)
hidden = F.silu(gate) * up

# å®é™…å¯ä»¥èåˆ gate_proj å’Œ up_proj
# å‡å°‘å†…å­˜è¯»å†™ï¼Œæé«˜æ•ˆç‡
gate_up = torch.cat([self.gate_proj(x), self.up_proj(x)], dim=-1)
gate, up = gate_up.chunk(2, dim=-1)
hidden = F.silu(gate) * up
```

---

## ğŸ“Š æ€§èƒ½è€ƒè™‘

### è®¡ç®—é‡åˆ†æ

```python
# FeedForward çš„ FLOPs
# å‡è®¾ batch=1, seq=512, hidden=512, intermediate=2048

# gate_proj: 512 Ã— 512 Ã— 2048 = 536M FLOPs
# up_proj:   512 Ã— 512 Ã— 2048 = 536M FLOPs
# down_proj: 512 Ã— 2048 Ã— 512 = 536M FLOPs
# å…ƒç´ ä¹˜æ³•:  512 Ã— 2048 â‰ˆ 1M FLOPs

# æ€»è®¡: â‰ˆ 1.6G FLOPs per block
```

**å¯¹æ¯” Attention**ï¼š
- Attention: â‰ˆ 1G FLOPsï¼ˆseq=512ï¼‰
- FeedForward: â‰ˆ 1.6G FLOPs
- FeedForward å ä¸»å¯¼ï¼ˆçº¦ 60%ï¼‰

---

### å†…å­˜ä½¿ç”¨

```python
# ä¸­é—´æ¿€æ´»å†…å­˜
# gate: batch Ã— seq Ã— intermediate = batch Ã— 512 Ã— 2048 floats
# up:   batch Ã— seq Ã— intermediate = batch Ã— 512 Ã— 2048 floats

# æ€»æ¿€æ´»å†…å­˜ â‰ˆ 2 Ã— batch Ã— 512 Ã— 2048 Ã— 4 bytes
#            = batch Ã— 8 MB
```

**ä¼˜åŒ–æŠ€å·§**ï¼š
- ä½¿ç”¨ checkpointingï¼šä¸ä¿å­˜ä¸­é—´æ¿€æ´»ï¼Œé‡æ–°è®¡ç®—
- æ··åˆç²¾åº¦ï¼šç”¨ BF16/FP16

---

## ğŸ”¬ å®éªŒéªŒè¯

### éªŒè¯ç»´åº¦å˜åŒ–

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleFeedForward(nn.Module):
    def __init__(self, dim=512, intermediate=2048):
        super().__init__()
        self.gate_proj = nn.Linear(dim, intermediate, bias=False)
        self.up_proj = nn.Linear(dim, intermediate, bias=False)
        self.down_proj = nn.Linear(intermediate, dim, bias=False)

    def forward(self, x):
        gate = self.gate_proj(x)
        print(f"gate: {gate.shape}")

        up = self.up_proj(x)
        print(f"up: {up.shape}")

        hidden = F.silu(gate) * up
        print(f"hidden: {hidden.shape}")

        output = self.down_proj(hidden)
        print(f"output: {output.shape}")

        return output

# æµ‹è¯•
ffn = SimpleFeedForward()
x = torch.randn(2, 10, 512)  # [batch=2, seq=10, dim=512]
print(f"input: {x.shape}")
output = ffn(x)
```

### éªŒè¯é—¨æ§æ•ˆæœ

```python
# å¯è§†åŒ–é—¨æ§ä¿¡å·
import matplotlib.pyplot as plt

x = torch.randn(1, 5, 512)  # 5 ä¸ª token
gate = F.silu(ffn.gate_proj(x))  # [1, 5, 2048]

# æŸ¥çœ‹ä¸åŒ token çš„é—¨æ§æ¿€æ´»
plt.figure(figsize=(10, 4))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.hist(gate[0, i].detach().numpy(), bins=50)
    plt.title(f"Token {i}")
plt.suptitle("Gate Activations")
plt.show()
```

---

## ğŸ”— ç›¸å…³ä»£ç ä½ç½®

1. **é…ç½®å‚æ•°**ï¼š`model/model_minimind.py:30-65`
   - `intermediate_size`ï¼šä¸­é—´ç»´åº¦
   - `hidden_size`ï¼šéšè—ç»´åº¦

2. **MoE FeedForward**ï¼š`model/model_minimind.py:380-450`
   - ä¸“å®¶æ··åˆç‰ˆæœ¬
   - æ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ª FeedForward

3. **å®Œæ•´ TransformerBlock**ï¼š`model/model_minimind.py:450-500`
   - Attention + FFN çš„ç»„åˆ

---

## ğŸ¯ åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå¯¹æ¯”æ¿€æ´»å‡½æ•°

å®ç°ä¸åŒæ¿€æ´»å‡½æ•°çš„ FFNï¼Œå¯¹æ¯”è¾“å‡ºåˆ†å¸ƒï¼š
```python
def ffn_relu(x):
    return W2(F.relu(W1(x)))

def ffn_gelu(x):
    return W2(F.gelu(W1(x)))

def ffn_swiglu(x):
    return W_down(F.silu(W_gate(x)) * W_up(x))
```

### ç»ƒä¹  2ï¼šå¯è§†åŒ–é—¨æ§

ä¿®æ”¹ä»£ç ï¼Œä¿å­˜å¹¶å¯è§†åŒ–é—¨æ§ä¿¡å·ï¼š
```python
# åœ¨ forward ä¸­ä¿å­˜
self.last_gate = F.silu(self.gate_proj(x))

# ç»˜åˆ¶çƒ­åŠ›å›¾
plt.imshow(model.ffn.last_gate[0].detach().numpy())
```

### ç»ƒä¹  3ï¼šè®¡ç®—å®é™… FLOPs

ç¼–å†™ä»£ç è®¡ç®— FeedForward çš„å®é™…è®¡ç®—é‡ï¼š
```python
from thop import profile
flops, params = profile(ffn, inputs=(x,))
print(f"FLOPs: {flops/1e6:.2f}M, Params: {params/1e6:.2f}M")
```

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- MiniMind å®Œæ•´ä»£ç ï¼š`model/model_minimind.py`
- Llama 2 æºç ï¼š[facebookresearch/llama](https://github.com/facebookresearch/llama)
- GLU è®ºæ–‡ï¼š[arXiv:2002.05202](https://arxiv.org/abs/2002.05202)
