# Position Encoding 自测题

> 完成以下题目检验你的理解程度

---

## 📝 选择题

### Q1: 为什么 Attention 需要位置编码？

A. 为了加速计算
B. 为了减少参数量
C. 因为 Attention 本身是排列不变的，无法区分顺序
D. 为了支持更长的序列

<details>
<summary>点击查看答案</summary>

**答案：C**

**解析**：
- Self-Attention 计算时，只关心"谁和谁相关"
- 不关心"谁在前谁在后"
- 例如 "我喜欢你" 和 "你喜欢我" 会产生相同的注意力模式
- 位置编码让模型能区分不同顺序

</details>

---

### Q2: RoPE 的核心思想是什么？

A. 给每个位置分配一个可学习的向量
B. 通过旋转向量来编码位置
C. 计算两个词之间的相对距离
D. 使用正弦函数生成位置编码

<details>
<summary>点击查看答案</summary>

**答案：B**

**解析**：
- RoPE = Rotary Position Embedding（旋转位置编码）
- 核心思想：位置 m → 旋转 mθ 角度
- 通过旋转角度差自动产生相对位置信息
- 与绝对位置编码（A）和原始 Transformer 的正弦编码（D）不同

</details>

---

### Q3: 为什么 RoPE 使用多个不同的频率？

A. 为了加速计算
B. 为了减少内存使用
C. 高频编码局部位置，低频编码全局位置
D. 为了兼容不同的 head_dim

<details>
<summary>点击查看答案</summary>

**答案：C**

**解析**：

**钟表类比**：
- 秒针（高频）：精确到秒（局部位置）
- 时针（低频）：精确到时（全局位置）
- 组合起来才能表示完整时间

**单频率的问题**：
- 只用高频：长序列时位置信息模糊（转太多圈）
- 只用低频：短序列时区分度不够（转太慢）

**多频率组合**：可以唯一标识百万级位置

</details>

---

### Q4: RoPE 应用在 Attention 的哪些部分？

A. 只有 Query
B. 只有 Key
C. Query 和 Key
D. Query、Key 和 Value

<details>
<summary>点击查看答案</summary>

**答案：C**

**解析**：

```python
# MiniMind 代码
xq, xk = apply_rotary_emb(xq, xk, freqs_cis)
# V 不需要！
```

**为什么 V 不需要？**
- Q 和 K 用于计算注意力分数（需要知道位置关系）
- V 是被查询的内容（位置信息已通过 Q·K 融入）
- 对 V 应用 RoPE 是多余的

</details>

---

### Q5: RoPE 相比绝对位置编码的主要优势是什么？

A. 计算更快
B. 参数更少
C. 支持长度外推（训练短序列，推理长序列）
D. 实现更简单

<details>
<summary>点击查看答案</summary>

**答案：C**

**解析**：

**绝对位置编码**：
- 每个位置一个向量：`pos_embed[0], pos_embed[1], ..., pos_embed[511]`
- 训练时最长 512 → 推理时遇到位置 600 没有对应编码
- ❌ 无法外推

**RoPE**：
- 只是旋转角度：位置 600 → 旋转 600θ
- 数学上可以计算任意位置
- ✅ 支持外推（配合 YaRN 效果更好）

</details>

---

## 🎯 综合问答题

### Q6: 实战问题

假设你训练了一个 RoPE 模型，max_seq_len=512。现在需要处理 2048 长度的文本，可能会遇到什么问题？如何解决？

<details>
<summary>点击查看参考答案</summary>

**可能的问题**：

1. **性能下降**：
   - 虽然数学上可以外推，但模型没见过这么长的序列
   - 长距离的注意力模式可能不准确

2. **旋转角度过大**：
   - 位置 2000 的旋转角度是训练时的 4 倍
   - 高频维度可能"转太多圈"，信息丢失

**解决方案**：

1. **使用 YaRN**（推荐）：
   ```python
   # 在 MiniMind 中启用
   config.inference_rope_scaling = True
   ```
   - 动态调整旋转频率
   - 让长序列的旋转更"缓和"

2. **Position Interpolation**：
   ```python
   # 将位置缩放到训练范围内
   pos_ids = pos_ids * (512 / 2048)
   ```
   - 简单但有效
   - 相当于"压缩"位置范围

3. **继续训练**：
   - 用长序列数据微调模型
   - 让模型适应长距离注意力

**最佳实践**：
- 训练时尽量用大的 rope_base（如 1e6）
- 推理时启用 YaRN
- 关键场景做测试验证

</details>

---

### Q7: 概念理解

用你自己的话解释：为什么 RoPE 的点积只依赖相对位置？

<details>
<summary>点击查看参考答案</summary>

**数学直觉**：

假设：
- 位置 m 的 Query：$q_m = R(m\theta) \cdot q$（旋转 mθ 角度）
- 位置 n 的 Key：$k_n = R(n\theta) \cdot k$（旋转 nθ 角度）

点积：
$$q_m \cdot k_n = [R(m\theta) \cdot q] \cdot [R(n\theta) \cdot k]$$

**关键性质**：旋转矩阵的转置等于逆旋转

$$= q \cdot R(-m\theta) \cdot R(n\theta) \cdot k$$
$$= q \cdot R((n-m)\theta) \cdot k$$

**结论**：
- 点积只包含 $(n-m)\theta$，即相对距离
- 不包含绝对位置 $m$ 或 $n$

**直觉类比**：
- 两个人面对面站着
- 无论他们站在房间的哪个位置
- 他们之间的"相对角度"是一样的

这就是为什么：
- 位置 (5, 8) 和位置 (100, 103) 的注意力分数相同
- 模型学到的是"相隔 3 个位置"的模式

</details>

---

## ✅ 完成检查

完成所有题目后，检查你是否达到：

- [ ] **Q1-Q5 全对**：基础知识扎实
- [ ] **Q6 能提出 2+ 解决方案**：具备实战能力
- [ ] **Q7 能清晰解释数学原理**：深刻理解概念

如果还有不清楚的地方，回到 [teaching.md](./teaching.md) 复习，或重新运行实验代码。

---

## 🎓 进阶挑战

想要更深入理解？尝试：

1. **修改实验代码**：
   - 实现一个简单的绝对位置编码
   - 对比 RoPE 的长度外推能力
   - 测试不同 rope_base 的效果

2. **阅读论文**：
   - [RoFormer 原始论文](https://arxiv.org/abs/2104.09864)
   - [YaRN 长度外推](https://arxiv.org/abs/2309.00071)

3. **实现变体**：
   - 实现 ALiBi（另一种位置编码）
   - 对比 RoPE vs ALiBi

---

**下一步**：前往 [03. Attention](../../03-attention) 学习注意力机制！
