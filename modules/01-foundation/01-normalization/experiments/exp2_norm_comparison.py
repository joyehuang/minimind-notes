"""
å®éªŒ 2ï¼šå››ç§é…ç½®å¯¹æ¯”

ç›®çš„ï¼šå¯¹æ¯” NoNormã€Post-LNã€Pre-LN ä¸‰ç§æ¶æ„çš„è®­ç»ƒæ•ˆæœ
æ–¹æ³•ï¼šè®­ç»ƒ 4 ä¸ªä¸åŒé…ç½®çš„ç®€åŒ– Transformer æ¨¡å‹
æ•°æ®ï¼šåˆæˆæ•°æ®ï¼ˆnext-token predictionï¼‰
æ—¶é—´ï¼š~3 åˆ†é’Ÿï¼ˆquick æ¨¡å¼ï¼š~30 ç§’ï¼‰
è¾“å‡ºï¼šresults/norm_comparison.png

æ³¨æ„ï¼šä¸ºä¿æŒå®éªŒç‹¬ç«‹æ€§ï¼Œæœ¬æ–‡ä»¶åŒ…å«ä¸ exp3 é‡å¤çš„åŸºç¡€ç±»ï¼ˆRMSNormã€Block ç­‰ï¼‰ï¼Œ
     ä¾¿äºå­¦ä¹ è€…å•ç‹¬è¿è¡Œå’Œç†è§£æ¯ä¸ªå®éªŒã€‚

è¿è¡Œï¼š
    python exp2_norm_comparison.py
    # å¿«é€Ÿæ¨¡å¼ï¼š
    python exp2_norm_comparison.py --quick
"""

import sys
sys.path.append('../../..')

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


# ============================================================
# RMSNorm å®ç°
# ============================================================
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)


# ============================================================
# ç®€åŒ–çš„ Transformer Block å®ç°
# ============================================================
class NoNormBlock(nn.Module):
    """æ— å½’ä¸€åŒ–çš„ Transformer Block"""
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )

    def forward(self, x):
        # Attention + Residual (æ— å½’ä¸€åŒ–)
        attn_out, _ = self.attention(x, x, x)
        x = x + attn_out

        # FFN + Residual (æ— å½’ä¸€åŒ–)
        x = x + self.ffn(x)
        return x


class PostLNBlock(nn.Module):
    """Post-LN: Compute â†’ Residual â†’ Normï¼ˆå½’ä¸€åŒ–åœ¨æ®‹å·®ä¹‹åï¼‰

    æ³¨æ„ï¼šæœ¬å®éªŒä¸­ Post-LN ä»…ä½¿ç”¨ LayerNormï¼Œå› ä¸ºå®éªŒç›®çš„æ˜¯å¯¹æ¯”æ¶æ„å·®å¼‚ï¼Œ
         è€Œéå½’ä¸€åŒ–æ–¹æ³•å·®å¼‚ï¼ˆRMSNorm ä»…åœ¨ Pre-LN ä¸­å¯¹æ¯”ï¼‰ã€‚
    """
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x):
        # Attention â†’ Residual â†’ Norm
        attn_out, _ = self.attention(x, x, x)
        x = x + attn_out
        x = self.norm1(x)

        # FFN â†’ Residual â†’ Norm
        x = x + self.ffn(x)
        x = self.norm2(x)
        return x


class PreLNBlock(nn.Module):
    """Pre-LN: Norm â†’ Compute â†’ Residualï¼ˆå½’ä¸€åŒ–åœ¨è®¡ç®—ä¹‹å‰ï¼‰"""
    def __init__(self, hidden_size, use_rms=False):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )

        if use_rms:
            self.norm1 = RMSNorm(hidden_size)
            self.norm2 = RMSNorm(hidden_size)
        else:
            self.norm1 = nn.LayerNorm(hidden_size)
            self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x):
        # Norm â†’ Attention â†’ Residual
        normed = self.norm1(x)
        attn_out, _ = self.attention(normed, normed, normed)
        x = x + attn_out

        # Norm â†’ FFN â†’ Residual
        normed = self.norm2(x)
        x = x + self.ffn(normed)
        return x


# ============================================================
# ç®€åŒ–çš„è¯­è¨€æ¨¡å‹
# ============================================================
class SimpleLM(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, block_type, use_rms=False):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)

        # æ ¹æ®é…ç½®é€‰æ‹© Block ç±»å‹
        if block_type == 'nonorm':
            self.blocks = nn.ModuleList([NoNormBlock(hidden_size) for _ in range(num_layers)])
        elif block_type == 'postln':
            # Post-LN å›ºå®šä½¿ç”¨ LayerNormï¼ˆä¸ä½¿ç”¨ use_rms å‚æ•°ï¼‰
            self.blocks = nn.ModuleList([PostLNBlock(hidden_size) for _ in range(num_layers)])
        elif block_type == 'preln':
            # Pre-LN å¯æ ¹æ® use_rms å‚æ•°é€‰æ‹© LayerNorm æˆ– RMSNorm
            self.blocks = nn.ModuleList([PreLNBlock(hidden_size, use_rms) for _ in range(num_layers)])

        self.lm_head = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embedding(x)

        for block in self.blocks:
            x = block(x)

        logits = self.lm_head(x)
        return logits


# ============================================================
# è®­ç»ƒå‡½æ•°
# ============================================================
def train_model(model, vocab_size, steps, lr, device):
    """è®­ç»ƒæ¨¡å‹å¹¶è¿”å›æŸå¤±æ›²çº¿

    æ³¨æ„ï¼šæœ¬å®éªŒæœªä½¿ç”¨æ¢¯åº¦è£å‰ªï¼Œç›®çš„æ˜¯å……åˆ†å±•ç¤ºä¸åŒæ¶æ„çš„åŸå§‹è®­ç»ƒç¨³å®šæ€§å·®å¼‚ã€‚
         NoNorm é…ç½®ä¼šå› æ¢¯åº¦çˆ†ç‚¸è€Œå‘æ•£ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦è§‚å¯Ÿçš„ç°è±¡ã€‚
    """
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    losses = []
    nan_step = None

    batch_size = 16
    seq_len = 64

    for step in range(steps):
        # ç”Ÿæˆéšæœºæ•°æ®ï¼ˆnext-token predictionï¼‰
        # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„åˆæˆæ•°æ®ã€‚torch.roll åˆ›å»ºäº†å¾ªç¯ä¾èµ–ï¼ˆæœ€åä¸€ä¸ª token çš„ç›®æ ‡æ˜¯ç¬¬ä¸€ä¸ª tokenï¼‰ï¼Œ
        #      ä¸ä»£è¡¨çœŸå®çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚ä½†å¯¹äºå±•ç¤ºå½’ä¸€åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§çš„å½±å“ï¼Œè¿™ä¸ªç®€åŒ–æ˜¯è¶³å¤Ÿçš„ã€‚
        X = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
        Y = torch.roll(X, shifts=-1, dims=1)  # ç›®æ ‡æ˜¯ä¸‹ä¸€ä¸ª token

        # Forward
        logits = model(X)
        loss = criterion(logits.view(-1, vocab_size), Y.view(-1))

        # æ£€æµ‹ NaN
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"      âš ï¸  NaN detected at step {step}")
            nan_step = step
            # å¡«å……å‰©ä½™çš„æŸå¤±å€¼ä¸º NaN
            losses.extend([float('nan')] * (steps - step))
            break

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        # æ¯ 100 æ­¥æ‰“å°ä¸€æ¬¡
        if (step + 1) % 100 == 0 or step == 0:
            print(f"      Step {step+1:4d}: loss = {loss.item():.4f}")

    return losses, nan_step


# ============================================================
# å®éªŒä¸»å‡½æ•°
# ============================================================
def run_experiment(quick_mode=False):
    """è¿è¡Œå››ç§é…ç½®å¯¹æ¯”å®éªŒ"""

    print("="*70)
    print("ğŸ”¬ å®éªŒ 2: å››ç§é…ç½®å¯¹æ¯”")
    print("="*70)

    # è®¾ç½®å‚æ•°
    vocab_size = 1000
    hidden_size = 256
    num_layers = 2
    steps = 100 if quick_mode else 1000
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    torch.manual_seed(42)

    print(f"\nğŸ“Š å®éªŒè®¾ç½®:")
    print(f"  - è¯è¡¨å¤§å°: {vocab_size}")
    print(f"  - éšè—ç»´åº¦: {hidden_size}")
    print(f"  - å±‚æ•°: {num_layers}")
    print(f"  - è®­ç»ƒæ­¥æ•°: {steps}")
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - æ¨¡å¼: {'å¿«é€Ÿæ¨¡å¼ (100 æ­¥)' if quick_mode else 'æ ‡å‡†æ¨¡å¼ (1000 æ­¥)'}")

    # é…ç½®åˆ—è¡¨ï¼š(åç§°, Blockç±»å‹, å­¦ä¹ ç‡, æ˜¯å¦ä½¿ç”¨RMSNorm)
    # æ³¨æ„ï¼šNoNorm ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆ1e-4ï¼‰ï¼Œä½†ä»ä¼šå› æ•°å€¼ä¸ç¨³å®šè€Œå‘æ•£
    #      è¿™å±•ç¤ºäº†å³ä½¿è°¨æ…è°ƒå‚ï¼Œæ— å½’ä¸€åŒ–æ¶æ„ä¹Ÿéš¾ä»¥ç¨³å®šè®­ç»ƒ
    configs = [
        ("NoNorm", 'nonorm', 1e-4, False),
        ("Post-LN + LayerNorm", 'postln', 1e-4, False),
        ("Pre-LN + LayerNorm", 'preln', 5e-4, False),
        ("Pre-LN + RMSNorm", 'preln', 5e-4, True),
    ]

    results = {}

    # è®­ç»ƒæ‰€æœ‰é…ç½®
    for name, block_type, lr, use_rms in configs:
        print(f"\n{'='*70}")
        print(f"ğŸ”µ è®­ç»ƒ: {name}")
        print(f"{'='*70}")
        print(f"   å­¦ä¹ ç‡: {lr}")
        print(f"   å½’ä¸€åŒ–ç±»å‹: {'RMSNorm' if use_rms else 'LayerNorm' if block_type != 'nonorm' else 'None'}")

        model = SimpleLM(vocab_size, hidden_size, num_layers, block_type, use_rms)
        losses, nan_step = train_model(model, vocab_size, steps, lr, device)

        results[name] = {
            'losses': losses,
            'nan_step': nan_step,
            'lr': lr,
            'final_loss': losses[-1] if not np.isnan(losses[-1]) else float('inf')
        }

        if nan_step is not None:
            print(f"   âŒ è®­ç»ƒå‘æ•£äºæ­¥æ•° {nan_step}")
        else:
            print(f"   âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")

    # å¯è§†åŒ–
    plot_results(results, steps)

    # è¾“å‡ºæ€»ç»“
    print_summary(results)


# ============================================================
# å¯è§†åŒ–å‡½æ•°
# ============================================================
def plot_results(results, steps):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿å’Œå¯¹æ¯”è¡¨æ ¼"""

    print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

    fig = plt.figure(figsize=(16, 6))

    # å·¦å›¾ï¼šè®­ç»ƒæŸå¤±æ›²çº¿
    ax1 = plt.subplot(1, 2, 1)

    colors = ['red', 'orange', 'blue', 'green']
    markers = ['x', 'o', 's', '^']

    for (name, data), color, marker in zip(results.items(), colors, markers):
        losses = data['losses']
        x = list(range(len(losses)))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼ˆè·³è¿‡ NaNï¼‰
        valid_indices = [i for i, loss in enumerate(losses) if not np.isnan(loss)]
        valid_losses = [losses[i] for i in valid_indices]

        ax1.plot(valid_indices, valid_losses,
                color=color, marker=marker, markevery=max(1, len(valid_indices)//10),
                label=name, linewidth=2, markersize=4, alpha=0.8)

    # è®¾ç½®åæ ‡è½´å’Œæ ·å¼ï¼ˆåœ¨æ‰€æœ‰æ•°æ®ç»˜åˆ¶å®Œæˆåï¼‰
    ax1.set_xlabel('è®­ç»ƒæ­¥æ•°', fontsize=12)
    ax1.set_ylabel('æŸå¤±', fontsize=12)
    ax1.set_title('è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10, loc='upper right')
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')

    # æ ‡è®° NaN ç‚¹ï¼ˆåœ¨åæ ‡è½´è®¾ç½®å®Œæˆåï¼Œæ­¤æ—¶ ylim å·²ç¡®å®šï¼‰
    for (name, data), color in zip(results.items(), colors):
        if data['nan_step'] is not None:
            ax1.axvline(x=data['nan_step'], color=color, linestyle='--', alpha=0.3)
            ax1.text(data['nan_step'], ax1.get_ylim()[1] * 0.9,
                    f'NaN@{data["nan_step"]}',
                    rotation=90, va='top', color=color, fontsize=9)

    # å³å›¾ï¼šå¯¹æ¯”è¡¨æ ¼
    ax2 = plt.subplot(1, 2, 2)
    ax2.axis('off')

    # æ„å»ºè¡¨æ ¼æ•°æ®
    table_data = []
    headers = ['é…ç½®', 'æ”¶æ•›æ€§', 'NaNæ­¥æ•°', 'æœ€ç»ˆLoss', 'LRå®¹å¿åº¦']

    for name, data in results.items():
        converged = "âœ…" if data['nan_step'] is None else "âŒ"
        nan_step = f"{data['nan_step']}" if data['nan_step'] is not None else "-"
        final_loss = f"{data['final_loss']:.2f}" if not np.isinf(data['final_loss']) else "NaN"
        lr_tolerance = "å¾ˆä½" if data['lr'] <= 1e-5 else "ä½" if data['lr'] <= 1e-4 else "ä¸­" if data['lr'] <= 5e-4 else "é«˜"

        table_data.append([name, converged, nan_step, final_loss, lr_tolerance])

    # ç»˜åˆ¶è¡¨æ ¼
    table = ax2.table(cellText=table_data, colLabels=headers,
                     cellLoc='center', loc='center',
                     colWidths=[0.3, 0.15, 0.15, 0.15, 0.15])

    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.5)

    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(len(headers)):
        table[(0, i)].set_facecolor('#40466e')
        table[(0, i)].set_text_props(weight='bold', color='white')

    # è®¾ç½®è¡Œé¢œè‰²
    colors = ['#ffcccc', '#ffe5cc', '#cce5ff', '#ccffcc']
    for i, color in enumerate(colors):
        for j in range(len(headers)):
            table[(i+1, j)].set_facecolor(color)

    ax2.set_title('æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    output_dir = Path(__file__).parent / 'results'
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / 'norm_comparison.png'

    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {output_path}")

    plt.show()


# ============================================================
# æ€»ç»“å‡½æ•°
# ============================================================
def print_summary(results):
    """æ‰“å°å®éªŒæ€»ç»“"""

    print("\n" + "="*70)
    print("ğŸ“Š å®éªŒç»“æœ")
    print("="*70)

    for name, data in results.items():
        print(f"\n{'='*70}")
        print(f"ğŸ“Œ {name}")
        print(f"{'='*70}")
        print(f"  å­¦ä¹ ç‡: {data['lr']}")

        if data['nan_step'] is not None:
            print(f"  âŒ è®­ç»ƒå‘æ•£äºæ­¥æ•° {data['nan_step']}")
            print(f"  åŸå› : æ•°å€¼ä¸ç¨³å®šï¼Œæ¢¯åº¦çˆ†ç‚¸")
        else:
            print(f"  âœ… è®­ç»ƒæˆåŠŸå®Œæˆ")
            print(f"  æœ€ç»ˆæŸå¤±: {data['final_loss']:.4f}")

    print("\n" + "="*70)
    print("ğŸ¯ å…³é”®å‘ç°")
    print("="*70)
    print("""
1. NoNorm é…ç½®:
   âŒ æ— æ³•ç¨³å®šè®­ç»ƒï¼Œå³ä½¿ä½¿ç”¨æå°çš„å­¦ä¹ ç‡ (1e-5)
   åŸå› ï¼šæ¿€æ´»å€¼å’Œæ¢¯åº¦åœ¨æ·±å±‚ç½‘ç»œä¸­ä¸ç¨³å®š

2. Post-LN + LayerNorm:
   âœ… å¯ä»¥è®­ç»ƒï¼Œä½†éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡
   åŸå› ï¼šä¸»è·¯å¾„ä¸Šçš„æ¢¯åº¦æµç»å½’ä¸€åŒ–å±‚ï¼Œå­˜åœ¨æ•°å€¼ä¸ç¨³å®šé£é™©

3. Pre-LN + LayerNorm:
   âœ… è®­ç»ƒç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
   åŸå› ï¼šä¸»è·¯å¾„ä¸Šçš„æ¢¯åº¦ç›´æ¥é€šè¿‡æ®‹å·®è¿æ¥ï¼Œæ›´ç¨³å®š

4. Pre-LN + RMSNorm:
   âœ… è®­ç»ƒæœ€ç¨³å®šï¼Œæ•ˆæœä¸ Pre-LN + LayerNorm ç›¸å½“
   ä¼˜åŠ¿ï¼šè®¡ç®—æ›´å¿«ï¼ˆçœç•¥å‡å€¼è®¡ç®—å’Œåç½®é¡¹ï¼‰

ğŸ’¡ ç»“è®ºï¼š
   - Pre-LN æ¶æ„åœ¨ç°ä»£ LLM ä¸­æˆä¸ºæ ‡å‡†ï¼ˆGPT-3/LLaMA/MiniMindï¼‰
   - RMSNorm åœ¨ä¿æŒæ•ˆæœçš„åŒæ—¶æä¾›æ›´é«˜çš„è®¡ç®—æ•ˆç‡
    """)


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼ˆ100æ­¥ï¼‰')
    args = parser.parse_args()

    run_experiment(quick_mode=args.quick)

    print("\n" + "="*70)
    print("ğŸ’­ æ€è€ƒé¢˜")
    print("="*70)
    print("""
1. ä¸ºä»€ä¹ˆ NoNorm é…ç½®å³ä½¿ä½¿ç”¨å¾ˆå°çš„å­¦ä¹ ç‡ä¹Ÿä¼š NaNï¼Ÿ
   æç¤ºï¼šæŸ¥çœ‹å®éªŒ 1 çš„æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ç°è±¡

2. Post-LN å’Œ Pre-LN çš„å…³é”®åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
   æç¤ºï¼šè§‚å¯Ÿå½’ä¸€åŒ–å±‚åœ¨æ®‹å·®è¿æ¥ä¸­çš„ä½ç½®

3. ä¸ºä»€ä¹ˆ Pre-LN å¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼Ÿ
   æç¤ºï¼šæ€è€ƒæ¢¯åº¦åœ¨ä¸»è·¯å¾„ä¸Šçš„æµåŠ¨æ–¹å¼

4. RMSNorm ç›¸æ¯” LayerNorm èŠ‚çœäº†å“ªäº›è®¡ç®—ï¼Ÿ
   æç¤ºï¼šå¯¹æ¯”ä¸¤è€…çš„å…¬å¼ï¼ˆå‚è€ƒ teaching.mdï¼‰

5. å¦‚æœå¢åŠ åˆ° 8 å±‚ï¼Œç»“æœä¼šæœ‰ä»€ä¹ˆå˜åŒ–ï¼Ÿ
   æç¤ºï¼šè¿è¡Œå®éªŒ 3 æŸ¥çœ‹æ·±å±‚ç½‘ç»œçš„å¯¹æ¯”
    """)
