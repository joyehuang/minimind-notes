"""
å®éªŒ 3ï¼šPre-LN vs Post-LN æ·±åº¦å¯¹æ¯”

ç›®çš„ï¼šéªŒè¯ Pre-LN åœ¨æ·±å±‚ç½‘ç»œä¸­çš„ä¼˜åŠ¿
æ–¹æ³•ï¼šå¯¹æ¯” 4å±‚ vs 8å±‚ æ¨¡å‹åœ¨ Pre-LN å’Œ Post-LN æ¶æ„ä¸‹çš„è®­ç»ƒæ•ˆæœ
æ•°æ®ï¼šåˆæˆæ•°æ®ï¼ˆnext-token predictionï¼‰
æ—¶é—´ï¼š~5 åˆ†é’Ÿï¼ˆquick æ¨¡å¼ï¼š~1 åˆ†é’Ÿï¼‰
è¾“å‡ºï¼šresults/prenorm_vs_postnorm.png

æ³¨æ„ï¼šä¸ºä¿æŒå®éªŒç‹¬ç«‹æ€§ï¼Œæœ¬æ–‡ä»¶åŒ…å«ä¸ exp2 é‡å¤çš„åŸºç¡€ç±»ï¼ˆRMSNormã€Block ç­‰ï¼‰ï¼Œ
     ä¾¿äºå­¦ä¹ è€…å•ç‹¬è¿è¡Œå’Œç†è§£æ¯ä¸ªå®éªŒã€‚æœ¬å®éªŒä½¿ç”¨æ¢¯åº¦è£å‰ªä»¥å…³æ³¨æ¶æ„æœ¬èº«çš„
     ç¨³å®šæ€§å·®å¼‚ï¼Œè€Œéæç«¯çš„æ¢¯åº¦çˆ†ç‚¸ç°è±¡ã€‚

è¿è¡Œï¼š
    python exp3_prenorm_vs_postnorm.py
    # å¿«é€Ÿæ¨¡å¼ï¼š
    python exp3_prenorm_vs_postnorm.py --quick
"""

import sys
sys.path.append('../../..')

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


# ============================================================
# RMSNorm å®ç°
# ============================================================
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)


# ============================================================
# Transformer Block å®ç°
# ============================================================
class PostLNBlock(nn.Module):
    """Post-LN: Compute â†’ Residual â†’ Normï¼ˆå½’ä¸€åŒ–åœ¨æ®‹å·®ä¹‹åï¼‰"""
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x):
        # Attention â†’ Residual â†’ Norm
        attn_out, _ = self.attention(x, x, x)
        x = x + attn_out
        x = self.norm1(x)

        # FFN â†’ Residual â†’ Norm
        x = x + self.ffn(x)
        x = self.norm2(x)
        return x


class PreLNBlock(nn.Module):
    """Pre-LN: Norm â†’ Compute â†’ Residualï¼ˆå½’ä¸€åŒ–åœ¨è®¡ç®—ä¹‹å‰ï¼‰"""
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Linear(hidden_size * 4, hidden_size)
        )
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x):
        # Norm â†’ Attention â†’ Residual
        normed = self.norm1(x)
        attn_out, _ = self.attention(normed, normed, normed)
        x = x + attn_out

        # Norm â†’ FFN â†’ Residual
        normed = self.norm2(x)
        x = x + self.ffn(normed)
        return x


# ============================================================
# ç®€åŒ–çš„è¯­è¨€æ¨¡å‹
# ============================================================
class SimpleLM(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, use_preln):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)

        if use_preln:
            self.blocks = nn.ModuleList([PreLNBlock(hidden_size) for _ in range(num_layers)])
        else:
            self.blocks = nn.ModuleList([PostLNBlock(hidden_size) for _ in range(num_layers)])

        self.lm_head = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embedding(x)

        for block in self.blocks:
            x = block(x)

        logits = self.lm_head(x)
        return logits


# ============================================================
# è®­ç»ƒå‡½æ•°
# ============================================================
def train_model(model, vocab_size, steps, lr, device):
    """è®­ç»ƒæ¨¡å‹å¹¶è¿”å›æŸå¤±æ›²çº¿å’Œç¨³å®šæ€§æŒ‡æ ‡"""
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    losses = []
    nan_detected = False

    batch_size = 16
    seq_len = 64

    for step in range(steps):
        # ç”Ÿæˆéšæœºæ•°æ®ï¼ˆnext-token predictionï¼‰
        # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„åˆæˆæ•°æ®ã€‚torch.roll åˆ›å»ºäº†å¾ªç¯ä¾èµ–ï¼ˆæœ€åä¸€ä¸ª token çš„ç›®æ ‡æ˜¯ç¬¬ä¸€ä¸ª tokenï¼‰ï¼Œ
        #      ä¸ä»£è¡¨çœŸå®çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚ä½†å¯¹äºå±•ç¤ºä¸åŒæ¶æ„çš„è®­ç»ƒç¨³å®šæ€§å·®å¼‚ï¼Œè¿™ä¸ªç®€åŒ–æ˜¯è¶³å¤Ÿçš„ã€‚
        X = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
        Y = torch.roll(X, shifts=-1, dims=1)

        # Forward
        logits = model(X)
        loss = criterion(logits.view(-1, vocab_size), Y.view(-1))

        # æ£€æµ‹ NaN
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"      âš ï¸  NaN detected at step {step}")
            nan_detected = True
            losses.extend([float('nan')] * (steps - step))
            break

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé¿å…æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        losses.append(loss.item())

        # æ‰“å°è¿›åº¦
        if (step + 1) % 100 == 0 or step == 0:
            print(f"      Step {step+1:4d}: loss = {loss.item():.4f}")

    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆå–æœ€å N æ­¥çš„æ ‡å‡†å·®ï¼ŒN = min(100, æ€»æ­¥æ•°)ï¼‰
    valid_losses = [l for l in losses if not np.isnan(l)]
    if len(valid_losses) > 10:
        last_n = min(100, len(valid_losses))
        stability = np.std(valid_losses[-last_n:])
    else:
        stability = float('inf')

    return losses, nan_detected, stability


# ============================================================
# å®éªŒä¸»å‡½æ•°
# ============================================================
def run_experiment(quick_mode=False):
    """è¿è¡Œ Pre-LN vs Post-LN æ·±åº¦å¯¹æ¯”å®éªŒ"""

    print("="*70)
    print("ğŸ”¬ å®éªŒ 3: Pre-LN vs Post-LN æ·±åº¦å¯¹æ¯”")
    print("="*70)

    # è®¾ç½®å‚æ•°
    vocab_size = 1000
    hidden_size = 256
    steps = 200 if quick_mode else 1000
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    torch.manual_seed(42)

    print(f"\nğŸ“Š å®éªŒè®¾ç½®:")
    print(f"  - è¯è¡¨å¤§å°: {vocab_size}")
    print(f"  - éšè—ç»´åº¦: {hidden_size}")
    print(f"  - è®­ç»ƒæ­¥æ•°: {steps}")
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - æ¨¡å¼: {'å¿«é€Ÿæ¨¡å¼ (200 æ­¥)' if quick_mode else 'æ ‡å‡†æ¨¡å¼ (1000 æ­¥)'}")

    # é…ç½®åˆ—è¡¨ï¼š(åç§°, å±‚æ•°, æ˜¯å¦ä½¿ç”¨ Pre-LN, å­¦ä¹ ç‡)
    configs = [
        ("Pre-LN 4-Layer", 4, True, 1e-3),
        ("Post-LN 4-Layer", 4, False, 1e-4),
        ("Pre-LN 8-Layer", 8, True, 1e-3),
        ("Post-LN 8-Layer", 8, False, 1e-4),
    ]

    results = {}

    # è®­ç»ƒæ‰€æœ‰é…ç½®
    for name, num_layers, use_preln, lr in configs:
        print(f"\n{'='*70}")
        print(f"ğŸ”µ è®­ç»ƒ: {name}")
        print(f"{'='*70}")
        print(f"   å±‚æ•°: {num_layers}")
        print(f"   æ¶æ„: {'Pre-LN' if use_preln else 'Post-LN'}")
        print(f"   å­¦ä¹ ç‡: {lr}")

        model = SimpleLM(vocab_size, hidden_size, num_layers, use_preln)
        losses, nan_detected, stability = train_model(model, vocab_size, steps, lr, device)

        results[name] = {
            'losses': losses,
            'num_layers': num_layers,
            'use_preln': use_preln,
            'lr': lr,
            'nan_detected': nan_detected,
            'stability': stability,
            'final_loss': losses[-1] if not np.isnan(losses[-1]) else float('inf')
        }

        if nan_detected:
            print(f"   âŒ è®­ç»ƒå‘æ•£")
        else:
            print(f"   âœ… è®­ç»ƒå®Œæˆ")
            print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
            print(f"   ç¨³å®šæ€§ (std): {stability:.4f}")

    # å¯è§†åŒ–
    plot_results(results, steps)

    # è¾“å‡ºæ€»ç»“
    print_summary(results)


# ============================================================
# å¯è§†åŒ–å‡½æ•°
# ============================================================
def plot_results(results, steps):
    """ç»˜åˆ¶ 2x2 å¯¹æ¯”å›¾"""

    print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # å·¦ä¸Šï¼š4å±‚å¯¹æ¯”
    ax1 = axes[0, 0]
    plot_comparison(ax1, results, layer_filter=4, title="4å±‚æ¨¡å‹ï¼šPre-LN vs Post-LN")

    # å³ä¸Šï¼š8å±‚å¯¹æ¯”
    ax2 = axes[0, 1]
    plot_comparison(ax2, results, layer_filter=8, title="8å±‚æ¨¡å‹ï¼šPre-LN vs Post-LN")

    # å·¦ä¸‹ï¼šPre-LN æ·±åº¦å¯¹æ¯”
    ax3 = axes[1, 0]
    plot_depth_comparison(ax3, results, arch_filter=True, title="Pre-LNï¼š4å±‚ vs 8å±‚")

    # å³ä¸‹ï¼šPost-LN æ·±åº¦å¯¹æ¯”
    ax4 = axes[1, 1]
    plot_depth_comparison(ax4, results, arch_filter=False, title="Post-LNï¼š4å±‚ vs 8å±‚")

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    output_dir = Path(__file__).parent / 'results'
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / 'prenorm_vs_postnorm.png'

    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {output_path}")

    plt.show()


def plot_comparison(ax, results, layer_filter, title):
    """ç»˜åˆ¶æŒ‡å®šå±‚æ•°ä¸‹çš„ Pre-LN vs Post-LN å¯¹æ¯”"""

    colors = {'Pre-LN': 'green', 'Post-LN': 'orange'}
    markers = {'Pre-LN': 's', 'Post-LN': 'o'}

    for name, data in results.items():
        if data['num_layers'] != layer_filter:
            continue

        losses = data['losses']
        valid_indices = [i for i, loss in enumerate(losses) if not np.isnan(loss)]
        valid_losses = [losses[i] for i in valid_indices]

        arch_name = 'Pre-LN' if data['use_preln'] else 'Post-LN'
        color = colors[arch_name]
        marker = markers[arch_name]

        ax.plot(valid_indices, valid_losses,
               color=color, marker=marker, markevery=max(1, len(valid_indices)//10),
               label=f"{arch_name} (LR={data['lr']})",
               linewidth=2, markersize=4, alpha=0.8)

    ax.set_xlabel('è®­ç»ƒæ­¥æ•°', fontsize=11)
    ax.set_ylabel('æŸå¤±', fontsize=11)
    ax.set_title(title, fontsize=13, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')


def plot_depth_comparison(ax, results, arch_filter, title):
    """ç»˜åˆ¶æŒ‡å®šæ¶æ„ä¸‹çš„ 4å±‚ vs 8å±‚ å¯¹æ¯”"""

    colors = {4: 'blue', 8: 'red'}
    markers = {4: 'o', 8: '^'}

    for name, data in results.items():
        if data['use_preln'] != arch_filter:
            continue

        losses = data['losses']
        valid_indices = [i for i, loss in enumerate(losses) if not np.isnan(loss)]
        valid_losses = [losses[i] for i in valid_indices]

        num_layers = data['num_layers']
        color = colors[num_layers]
        marker = markers[num_layers]

        ax.plot(valid_indices, valid_losses,
               color=color, marker=marker, markevery=max(1, len(valid_indices)//10),
               label=f"{num_layers}å±‚",
               linewidth=2, markersize=4, alpha=0.8)

    ax.set_xlabel('è®­ç»ƒæ­¥æ•°', fontsize=11)
    ax.set_ylabel('æŸå¤±', fontsize=11)
    ax.set_title(title, fontsize=13, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')


# ============================================================
# æ€»ç»“å‡½æ•°
# ============================================================
def print_summary(results):
    """æ‰“å°å®éªŒæ€»ç»“"""

    print("\n" + "="*70)
    print("ğŸ“Š å®éªŒç»“æœ")
    print("="*70)

    for name, data in results.items():
        print(f"\n{'='*70}")
        print(f"ğŸ“Œ {name}")
        print(f"{'='*70}")
        print(f"  æ¶æ„: {'Pre-LN' if data['use_preln'] else 'Post-LN'}")
        print(f"  å±‚æ•°: {data['num_layers']}")
        print(f"  å­¦ä¹ ç‡: {data['lr']}")

        if data['nan_detected']:
            print(f"  âŒ è®­ç»ƒå‘æ•£")
        else:
            print(f"  âœ… è®­ç»ƒç¨³å®š")
            print(f"  æœ€ç»ˆæŸå¤±: {data['final_loss']:.4f}")
            print(f"  ç¨³å®šæ€§ (std): {data['stability']:.4f}")

    print("\n" + "="*70)
    print("ğŸ¯ å…³é”®å‘ç°")
    print("="*70)
    print("""
1. æµ…å±‚ç½‘ç»œï¼ˆ4å±‚ï¼‰:
   âœ… Pre-LN å’Œ Post-LN éƒ½èƒ½ç¨³å®šè®­ç»ƒ
   åŒºåˆ«ï¼šPre-LN å¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼ˆ1e-3 vs 1e-4ï¼‰

2. æ·±å±‚ç½‘ç»œï¼ˆ8å±‚ï¼‰:
   âœ… Pre-LN ä¾ç„¶ç¨³å®šï¼Œè®­ç»ƒæ›²çº¿å¹³æ»‘
   âš ï¸  Post-LN éœ€è¦æ›´å°å¿ƒçš„å­¦ä¹ ç‡è°ƒèŠ‚
   åŸå› ï¼šPost-LN çš„æ¢¯åº¦æµç»å½’ä¸€åŒ–å±‚ï¼Œåœ¨æ·±å±‚ç½‘ç»œä¸­æ›´å®¹æ˜“ä¸ç¨³å®š

3. å­¦ä¹ ç‡å®¹å¿åº¦:
   - Pre-LN: å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼ˆ1e-3ï¼‰ï¼Œè®­ç»ƒæ›´å¿«
   - Post-LN: éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆ1e-4ï¼‰ï¼Œè®­ç»ƒè¾ƒæ…¢

4. ç¨³å®šæ€§å¯¹æ¯”:
   - Pre-LN: æŸå¤±æ›²çº¿æ›´å¹³æ»‘ï¼Œæ ‡å‡†å·®æ›´å°
   - Post-LN: æŸå¤±æ›²çº¿æ³¢åŠ¨è¾ƒå¤§ï¼Œå°¤å…¶æ˜¯æ·±å±‚ç½‘ç»œ

ğŸ’¡ ç»“è®ºï¼š
   - Pre-LN åœ¨æ·±å±‚ç½‘ç»œä¸­å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿
   - ç°ä»£ LLMï¼ˆGPT-3/LLaMA/MiniMindï¼‰éƒ½é‡‡ç”¨ Pre-LN æ¶æ„
   - Pre-LN ä½¿å¾—è®­ç»ƒ 100+ å±‚çš„è¶…å¤§æ¨¡å‹æˆä¸ºå¯èƒ½
    """)


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼ˆ200æ­¥ï¼‰')
    args = parser.parse_args()

    run_experiment(quick_mode=args.quick)

    print("\n" + "="*70)
    print("ğŸ’­ æ€è€ƒé¢˜")
    print("="*70)
    print("""
1. ä¸ºä»€ä¹ˆ Pre-LN åœ¨æ·±å±‚ç½‘ç»œä¸­æ›´ç¨³å®šï¼Ÿ
   æç¤ºï¼šæ€è€ƒæ¢¯åº¦åœ¨æ®‹å·®è¿æ¥ä¸­çš„ä¼ æ’­è·¯å¾„

2. Post-LN ä¸ºä»€ä¹ˆéœ€è¦æ›´å°çš„å­¦ä¹ ç‡ï¼Ÿ
   æç¤ºï¼šå½’ä¸€åŒ–å±‚å¯¹æ¢¯åº¦çš„å½±å“

3. å¦‚æœå¢åŠ åˆ° 16 å±‚æˆ– 32 å±‚ï¼Œç»“æœä¼šæ€æ ·ï¼Ÿ
   æç¤ºï¼šPre-LN çš„ä¼˜åŠ¿ä¼šæ›´åŠ æ˜æ˜¾

4. Pre-LN æœ‰ä»€ä¹ˆç¼ºç‚¹å—ï¼Ÿ
   æç¤ºï¼šæœ€åä¸€å±‚çš„è¾“å‡ºæ²¡æœ‰ç»è¿‡å½’ä¸€åŒ–

5. ä¸ºä»€ä¹ˆç°ä»£ LLM éƒ½é€‰æ‹© Pre-LNï¼Ÿ
   æç¤ºï¼šç»“åˆç¨³å®šæ€§ã€å­¦ä¹ ç‡å®¹å¿åº¦ã€å¯æ‰©å±•æ€§ä¸‰ä¸ªæ–¹é¢è€ƒè™‘
    """)
