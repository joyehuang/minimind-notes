# 01. Normalizationï¼ˆå½’ä¸€åŒ–ï¼‰

> ä¸ºä»€ä¹ˆæ·±å±‚ç½‘ç»œéœ€è¦å½’ä¸€åŒ–ï¼ŸRMSNorm å¦‚ä½•ç¨³å®šè®­ç»ƒï¼Ÿ

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜çš„æœ¬è´¨
- âœ… ç†è§£å½’ä¸€åŒ–å¦‚ä½•ç¨³å®šæ¿€æ´»åˆ†å¸ƒ
- âœ… ç†è§£ RMSNorm vs LayerNorm çš„åŒºåˆ«
- âœ… ç†è§£ Pre-LN vs Post-LN çš„ä¼˜åŠ£
- âœ… ä»é›¶å®ç° RMSNorm

---

## ğŸ“š å­¦ä¹ è·¯å¾„

### 1ï¸âƒ£ å¿«é€Ÿä½“éªŒï¼ˆ10 åˆ†é’Ÿï¼‰

å…ˆè¿è¡Œå®éªŒï¼Œå»ºç«‹ç›´è§‰ï¼š

```bash
cd experiments

# å®éªŒ 1ï¼šçœ‹çœ‹æ²¡æœ‰å½’ä¸€åŒ–ä¼šå‘ç”Ÿä»€ä¹ˆ
python exp1_gradient_vanishing.py

# å®éªŒ 2ï¼šå¯¹æ¯”å››ç§é…ç½®
python exp2_norm_comparison.py --quick
```

**ä½ ä¼šçœ‹åˆ°**ï¼š
- æ— å½’ä¸€åŒ–ï¼šæ¿€æ´»å€¼æ ‡å‡†å·®è¿…é€Ÿè¡°å‡åˆ° 0ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰
- Post-LNï¼šè®­ç»ƒåˆæœŸ loss éœ‡è¡
- Pre-LN + RMSNormï¼šç¨³å®šæ”¶æ•› âœ…

---

### 2ï¸âƒ£ ç†è®ºå­¦ä¹ ï¼ˆ30 åˆ†é’Ÿï¼‰

é˜…è¯»æ•™å­¦æ–‡æ¡£ï¼š
- ğŸ“– [teaching.md](./teaching.md) - å®Œæ•´çš„æ¦‚å¿µè®²è§£

**æ ¸å¿ƒå†…å®¹**ï¼š
- **Why**: ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ
- **What**: RMSNorm æ˜¯ä»€ä¹ˆï¼Ÿ
- **How**: å¦‚ä½•éªŒè¯ï¼Ÿ

---

### 3ï¸âƒ£ ä»£ç å®ç°ï¼ˆ15 åˆ†é’Ÿï¼‰

æŸ¥çœ‹çœŸå®ä»£ç ï¼š
- ğŸ’» [code_guide.md](./code_guide.md) - MiniMind æºç å¯¼è¯»

**å…³é”®æ–‡ä»¶**ï¼š
- `model/model_minimind.py:95-105` - RMSNorm å®ç°
- `model/model_minimind.py:359-380` - TransformerBlock ä¸­çš„ä½¿ç”¨

---

### 4ï¸âƒ£ è‡ªæµ‹å·©å›ºï¼ˆ5 åˆ†é’Ÿï¼‰

å®Œæˆè‡ªæµ‹é¢˜ï¼š
- ğŸ“ [quiz.md](./quiz.md) - 5 é“é€‰æ‹©é¢˜

---

## ğŸ”¬ å®éªŒåˆ—è¡¨

| å®éªŒ | ç›®çš„ | æ—¶é—´ | æ•°æ® |
|------|------|------|------|
| exp1_gradient_vanishing.py | è¯æ˜å½’ä¸€åŒ–çš„å¿…è¦æ€§ | 10ç§’ | åˆæˆæ•°æ® |
| exp2_norm_comparison.py | å¯¹æ¯”å››ç§é…ç½® | 5åˆ†é’Ÿ | TinyShakespeare |
| exp3_prenorm_vs_postnorm.py | Pre-LN vs Post-LN | 8åˆ†é’Ÿ | TinyShakespeare |

### è¿è¡Œæ‰€æœ‰å®éªŒ

```bash
cd experiments
bash run_all.sh
```

---

## ğŸ’¡ å…³é”®è¦ç‚¹

### 1. ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ

```
æ— å½’ä¸€åŒ–çš„æ·±å±‚ç½‘ç»œï¼š
Layer 1: std = 1.00
Layer 2: std = 0.85
Layer 3: std = 0.62
Layer 4: std = 0.38
...
Layer 8: std = 0.016  â† æ¢¯åº¦å‡ ä¹æ¶ˆå¤±ï¼
```

**ç›´è§‰**ï¼šåƒæ°´é¾™å¤´è£…"æ°´å‹ç¨³å®šå™¨"ï¼Œæ— è®ºè¾“å…¥å¦‚ä½•å˜åŒ–ï¼Œè¾“å‡ºéƒ½ä¿æŒç¨³å®šã€‚

---

### 2. RMSNorm vs LayerNorm

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| è®¡ç®—æ­¥éª¤ | å‡å‡å€¼ + é™¤æ ‡å‡†å·® | é™¤ RMS |
| å‚æ•°é‡ | 2d (Î³, Î²) | d (Î³) |
| é€Ÿåº¦ | æ…¢ | å¿« 7-64% |
| åŠç²¾åº¦ç¨³å®šæ€§ | è¾ƒå·® | æ›´å¥½ |

**ç»“è®º**ï¼šRMSNorm æ›´ç®€å•ã€æ›´å¿«ã€æ›´ç¨³å®šã€‚

---

### 3. Pre-LN vs Post-LN

```python
# Post-LNï¼ˆæ—§æ–¹æ¡ˆï¼‰
x = x + Attention(x)          # å…ˆè®¡ç®—
x = LayerNorm(x)              # åå½’ä¸€åŒ–

# Pre-LNï¼ˆç°ä»£æ–¹æ¡ˆï¼‰
x = x + Attention(Norm(x))    # å…ˆå½’ä¸€åŒ–
x = x + FFN(Norm(x))          # å†è®¡ç®—
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ®‹å·®è·¯å¾„æ›´"å¹²å‡€"ï¼ˆæ¢¯åº¦ç›´è¾¾ï¼‰
- âœ… æ·±å±‚ç½‘ç»œï¼ˆ>12 å±‚ï¼‰æ›´ç¨³å®š
- âœ… å­¦ä¹ ç‡å®¹å¿åº¦æ›´é«˜

---

## ğŸ“Š é¢„æœŸç»“æœ

å®Œæˆå®éªŒåï¼Œä½ ä¼šåœ¨ `experiments/results/` çœ‹åˆ°ï¼š

1. **gradient_vanishing.png**
   - æ— å½’ä¸€åŒ–ï¼šæ ‡å‡†å·®è¡°å‡æ›²çº¿
   - æœ‰å½’ä¸€åŒ–ï¼šæ ‡å‡†å·®ä¿æŒç¨³å®š

2. **norm_comparison.png**
   - å››æ¡ loss æ›²çº¿å¯¹æ¯”
   - NoNorm åœ¨ 500 æ­¥å NaN
   - Pre-LN æœ€ç¨³å®š

3. **prenorm_vs_postnorm.png**
   - Pre-LN vs Post-LN çš„æ”¶æ•›é€Ÿåº¦å¯¹æ¯”

---

## âœ… å®Œæˆæ£€æŸ¥

å­¦å®Œæœ¬æ¨¡å—åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

### ç†è®º
- [ ] ç”¨è‡ªå·±çš„è¯è§£é‡Šæ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- [ ] è¯´å‡º RMSNorm çš„æ•°å­¦å…¬å¼
- [ ] è§£é‡Š Pre-LN ä¸ºä»€ä¹ˆæ¯” Post-LN å¥½

### å®è·µ
- [ ] ä»é›¶å®ç° RMSNorm
- [ ] èƒ½ç”»å‡º Pre-LN Transformer Block çš„æ•°æ®æµå›¾
- [ ] èƒ½è°ƒè¯•å½’ä¸€åŒ–ç›¸å…³çš„è®­ç»ƒé—®é¢˜

### ç›´è§‰
- [ ] èƒ½ç”¨ç±»æ¯”è§£é‡Šå½’ä¸€åŒ–çš„ä½œç”¨
- [ ] èƒ½é¢„æµ‹å»æ‰å½’ä¸€åŒ–ä¼šå‘ç”Ÿä»€ä¹ˆ
- [ ] èƒ½è§£é‡Šä¸ºä»€ä¹ˆç°ä»£ LLM é€‰æ‹© Pre-LN + RMSNorm

---

## ğŸ”— ç›¸å…³èµ„æº

### è®ºæ–‡
- [RMSNorm](https://arxiv.org/abs/1910.07467) - Root Mean Square Layer Normalization
- [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745) - Pre-LN vs Post-LN

### åšå®¢
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
- [Understanding Layer Normalization](https://leimao.github.io/blog/Layer-Normalization/)

### è§†é¢‘
- [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)

---

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ¨¡å—åï¼Œå‰å¾€ï¼š
ğŸ‘‰ [02. Position Encodingï¼ˆä½ç½®ç¼–ç ï¼‰](../02-position-encoding)

å­¦ä¹  Transformer å¦‚ä½•å¤„ç†ä½ç½®ä¿¡æ¯ï¼Œç†è§£ RoPE çš„åŸç†ã€‚
