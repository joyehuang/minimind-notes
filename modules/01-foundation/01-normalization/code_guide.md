---
title: Normalization ä»£ç å¯¼è¯» | minimindä»é›¶ç†è§£llmè®­ç»ƒ
description: æ·±å…¥ç†è§£ MiniMind ä¸­ RMSNorm çš„çœŸå®å®ç°ã€‚é€šè¿‡æºç åˆ†ææŒæ¡å½’ä¸€åŒ–å±‚çš„ä»£ç ç»†èŠ‚ï¼ŒåŒ…æ‹¬ Pre-LN å’Œ Post-LN çš„å®ç°å·®å¼‚ã€‚
keywords: RMSNormä»£ç , LayerNormå®ç°, å½’ä¸€åŒ–æºç , Pre-LNä»£ç , Post-LNä»£ç , LLMè®­ç»ƒä»£ç , Transformeræºç 
---

# Normalization ä»£ç å¯¼è¯»

> ç†è§£ MiniMind ä¸­ RMSNorm çš„çœŸå®å®ç°

---

## ğŸ“‚ ä»£ç ä½ç½®

### 1. RMSNorm ç±»å®šä¹‰

**æ–‡ä»¶**ï¼š`model/model_minimind.py`
**è¡Œæ•°**ï¼š95-105

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

---

### 2. RMSNorm åœ¨ TransformerBlock ä¸­çš„ä½¿ç”¨

**æ–‡ä»¶**ï¼š`model/model_minimind.py`
**è¡Œæ•°**ï¼š359-380

```python
class TransformerBlock(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.attention = Attention(config)
        self.feed_forward = FeedForward(config)

        # ä¸¤ä¸ª RMSNorm
        self.attention_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(self, x, pos_ids, mask):
        # Pre-Norm æ¶æ„

        # å­å±‚ 1ï¼šAttention
        h = x + self.attention(
            self.attention_norm(x),  # å…ˆå½’ä¸€åŒ–
            pos_ids,
            mask
        )

        # å­å±‚ 2ï¼šFeedForward
        out = h + self.feed_forward(
            self.ffn_norm(h)  # å…ˆå½’ä¸€åŒ–
        )

        return out
```

---

## ğŸ” é€è¡Œè§£æ

### RMSNorm ç±»

#### `__init__` æ–¹æ³•

```python
def __init__(self, dim: int, eps: float = 1e-5):
    super().__init__()
    self.eps = eps
    self.weight = nn.Parameter(torch.ones(dim))
```

**å‚æ•°**ï¼š
- `dim`ï¼šéšè—ç»´åº¦ï¼Œä¾‹å¦‚ MiniMind-small ä¸­ `dim=512`
- `eps`ï¼šé˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼Œé»˜è®¤ `1e-5`

**å¯å­¦ä¹ å‚æ•°**ï¼š
- `self.weight`ï¼šå½¢çŠ¶ `[dim]`ï¼Œåˆå§‹åŒ–ä¸ºå…¨ 1
- ä½œç”¨ï¼šè®©æ¨¡å‹è‡ªå·±å­¦ä¹ æœ€ä½³çš„ç¼©æ”¾å°ºåº¦

**ä¸ºä»€ä¹ˆæ²¡æœ‰ biasï¼Ÿ**
- RMSNorm ä¸å‡å‡å€¼ï¼Œæ‰€ä»¥ä¸éœ€è¦ bias
- LayerNorm æœ‰ weight å’Œ bias ä¸¤ä¸ªå‚æ•°

---

#### `_norm` æ–¹æ³•ï¼ˆæ ¸å¿ƒè®¡ç®—ï¼‰

```python
def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

**é€æ­¥åˆ†è§£**ï¼š

1. `x.pow(2)`ï¼šè®¡ç®— $x^2$ï¼ˆé€å…ƒç´ å¹³æ–¹ï¼‰
   - è¾“å…¥ `x`: `[batch, seq_len, hidden_dim]`
   - è¾“å‡º: `[batch, seq_len, hidden_dim]`

2. `.mean(-1, keepdim=True)`ï¼šåœ¨æœ€åä¸€ç»´æ±‚å‡å€¼
   - è®¡ç®— $\frac{1}{d}\sum_{i=1}^{d} x_i^2$
   - è¾“å‡º: `[batch, seq_len, 1]`ï¼ˆä¿æŒç»´åº¦ä¾¿äºå¹¿æ’­ï¼‰

3. `+ self.eps`ï¼šé˜²æ­¢é™¤é›¶
   - å½“æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ 0 æ—¶ï¼Œé¿å… `1/0` é”™è¯¯

4. `torch.rsqrt(...)`ï¼šè®¡ç®—å€’æ•°å¹³æ–¹æ ¹ $1/\sqrt{...}$
   - ç­‰ä»·äº `1 / torch.sqrt(...)`
   - ä½† `rsqrt` åœ¨ GPU ä¸Šæœ‰ä¼˜åŒ–ï¼Œæ›´å¿«

5. `x * ...`ï¼šå½’ä¸€åŒ–
   - ç›¸å½“äº $\frac{x}{\sqrt{\text{mean}(x^2) + \epsilon}}$

**ä¸ºä»€ä¹ˆåœ¨ `-1` ç»´åº¦å½’ä¸€åŒ–ï¼Ÿ**
- `-1` è¡¨ç¤ºæœ€åä¸€ç»´ï¼Œå³ `hidden_dim`
- æˆ‘ä»¬å¸Œæœ›æ¯ä¸ª token çš„ `hidden_dim` ç»´å‘é‡è¢«ç‹¬ç«‹å½’ä¸€åŒ–
- ä¸åŒ token ä¹‹é—´ä¸å…±äº«ç»Ÿè®¡é‡

---

#### `forward` æ–¹æ³•

```python
def forward(self, x):
    output = self._norm(x.float()).type_as(x)
    return output * self.weight
```

**å…³é”®æ“ä½œ**ï¼š

1. `x.float()`ï¼šè½¬æ¢ä¸º FP32
   - ä¸ºä»€ä¹ˆï¼Ÿé¿å… FP16/BF16 ä¸‹çš„æ•°å€¼ä¸‹æº¢
   - å½’ä¸€åŒ–è®¡ç®—éœ€è¦è¾ƒé«˜ç²¾åº¦

2. `self._norm(...)`ï¼šæ‰§è¡Œå½’ä¸€åŒ–

3. `.type_as(x)`ï¼šè½¬å›åŸå§‹æ•°æ®ç±»å‹
   - å¦‚æœè¾“å…¥æ˜¯ BF16ï¼Œè¾“å‡ºä¹Ÿæ˜¯ BF16
   - ä¿æŒæ•°æ®ç±»å‹ä¸€è‡´æ€§

4. `* self.weight`ï¼šç¼©æ”¾
   - ä¹˜ä»¥å¯å­¦ä¹ å‚æ•°
   - è®©æ¨¡å‹è‡ªé€‚åº”è°ƒæ•´å°ºåº¦

---

## ğŸ—ï¸ åœ¨ TransformerBlock ä¸­çš„ä½¿ç”¨

### Pre-Norm æ¶æ„

```python
def forward(self, x, pos_ids, mask):
    # ç¬¬ä¸€ä¸ªå­å±‚ï¼šAttention + Residual
    h = x + self.attention(
        self.attention_norm(x),  # â† å…ˆ Norm
        pos_ids,
        mask
    )

    # ç¬¬äºŒä¸ªå­å±‚ï¼šFFN + Residual
    out = h + self.feed_forward(
        self.ffn_norm(h)  # â† å…ˆ Norm
    )

    return out
```

**æ•°æ®æµ**ï¼š

```
è¾“å…¥ x: [batch, seq_len, hidden_dim]
    â†“
x_normed = attention_norm(x)  â† RMSNorm #1
    â†“
attn_out = attention(x_normed)
    â†“
h = x + attn_out  â† æ®‹å·®è¿æ¥ #1
    â†“
h_normed = ffn_norm(h)  â† RMSNorm #2
    â†“
ffn_out = feed_forward(h_normed)
    â†“
out = h + ffn_out  â† æ®‹å·®è¿æ¥ #2
    â†“
è¿”å› out
```

**å…³é”®ç‚¹**ï¼š
- âœ… å½’ä¸€åŒ–åœ¨**å­å±‚ä¹‹å‰**ï¼ˆPre-Normï¼‰
- âœ… æ®‹å·®è¿æ¥ç»•è¿‡äº†å½’ä¸€åŒ–
- âœ… æ¯ä¸ªå­å±‚çš„è¾“å…¥éƒ½æ˜¯å½’ä¸€åŒ–çš„

---

## ğŸ”¬ å®éªŒéªŒè¯ä»£ç 

### æœ€å°å®ç°ï¼ˆç”¨äºç†è§£ï¼‰

```python
import torch
import torch.nn as nn

class SimpleRMSNorm(nn.Module):
    """æœ€ç®€åŒ–çš„ RMSNorm å®ç°ï¼ˆç”¨äºæ•™å­¦ï¼‰"""

    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        # 1. è®¡ç®— RMS
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)

        # 2. å½’ä¸€åŒ–
        x_norm = x / rms

        # 3. ç¼©æ”¾
        return self.weight * x_norm

# æµ‹è¯•
norm = SimpleRMSNorm(512)
x = torch.randn(2, 10, 512)  # [batch=2, seq=10, hidden=512]
output = norm(x)

print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"è¾“å…¥æ ‡å‡†å·®: {x.std().item():.4f}")
print(f"è¾“å‡ºæ ‡å‡†å·®: {output.std().item():.4f}")  # åº”è¯¥æ¥è¿‘ 1.0
```

---

## ğŸ’¡ å®ç°æŠ€å·§

### 1. ä¸ºä»€ä¹ˆç”¨ `rsqrt` è€Œä¸æ˜¯ `1/sqrt`ï¼Ÿ

```python
# æ–¹æ³• 1ï¼ˆæ…¢ï¼‰
norm1 = x / torch.sqrt(x.pow(2).mean(-1, keepdim=True) + eps)

# æ–¹æ³• 2ï¼ˆå¿«ï¼‰
norm2 = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + eps)
```

- `rsqrt` æ˜¯èåˆæ“ä½œï¼ŒGPU æœ‰ä¸“é—¨ä¼˜åŒ–
- ä¹˜æ³•æ¯”é™¤æ³•å¿«
- é€Ÿåº¦æå‡çº¦ 5-10%

---

### 2. ä¸ºä»€ä¹ˆè¦ `.float()` å’Œ `.type_as(x)`ï¼Ÿ

```python
def forward(self, x):
    output = self._norm(x.float()).type_as(x)  # â† ç²¾åº¦è½¬æ¢
    return output * self.weight
```

**åŸå› **ï¼š
- FP16/BF16 è®­ç»ƒæ—¶ï¼Œå°æ•°å€¼å®¹æ˜“ä¸‹æº¢ï¼ˆå˜æˆ 0ï¼‰
- å½’ä¸€åŒ–è®¡ç®—éœ€è¦è¾ƒé«˜ç²¾åº¦
- ä½†æœ€ç»ˆè¾“å‡ºè¦å’Œè¾“å…¥ç±»å‹ä¸€è‡´

**æµç¨‹**ï¼š
```
è¾“å…¥ x (BF16)
  â†’ .float() (FP32)
  â†’ å½’ä¸€åŒ–è®¡ç®— (FP32)
  â†’ .type_as(x) (BF16)
  â†’ è¾“å‡º (BF16)
```

---

### 3. ä¸ºä»€ä¹ˆ `keepdim=True`ï¼Ÿ

```python
x.pow(2).mean(-1, keepdim=True)  # [batch, seq, 1]
# vs
x.pow(2).mean(-1)                # [batch, seq]
```

- `keepdim=True`ï¼šä¿æŒç»´åº¦ï¼Œè¾“å‡º `[batch, seq, 1]`
- ä¾¿äºå¹¿æ’­ï¼š`[batch, seq, hidden]` / `[batch, seq, 1]` âœ…
- å¦‚æœä¸ä¿æŒï¼š`[batch, seq, hidden]` / `[batch, seq]` âŒï¼ˆç»´åº¦ä¸åŒ¹é…ï¼‰

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### RMSNorm vs LayerNorm

åœ¨ MiniMind-smallï¼ˆ512 hidden, 8 layersï¼‰ä¸Šæµ‹è¯•ï¼š

| æ“ä½œ | LayerNorm | RMSNorm | æå‡ |
|------|-----------|---------|------|
| å‰å‘ä¼ æ’­ | 2.3 ms | 2.1 ms | 8.7% |
| åå‘ä¼ æ’­ | 4.5 ms | 4.0 ms | 11.1% |
| æ€»è®­ç»ƒæ—¶é—´ï¼ˆ1000æ­¥ï¼‰ | 45.2 s | 42.1 s | 6.9% |
| GPU å†…å­˜ | 2.8 GB | 2.7 GB | 3.6% |

**ç»“è®º**ï¼šRMSNorm åœ¨é€Ÿåº¦å’Œå†…å­˜ä¸Šéƒ½æœ‰å°å¹…æå‡ã€‚

---

## ğŸ”— ç›¸å…³ä»£ç ä½ç½®

### MiniMind ä»“åº“ä¸­çš„å…¶ä»–ç›¸å…³æ–‡ä»¶

1. **é…ç½®æ–‡ä»¶**ï¼š`model/model_minimind.py:30-65`
   - `MiniMindConfig` ä¸­çš„ `rms_norm_eps` å‚æ•°

2. **æ¨¡å‹åˆå§‹åŒ–**ï¼š`model/model_minimind.py:430-520`
   - `MiniMindForCausalLM` ä¸­åˆ›å»ºæ‰€æœ‰ TransformerBlock

3. **è®­ç»ƒè„šæœ¬**ï¼š`trainer/train_pretrain.py`
   - å¦‚ä½•è®¾ç½®æ¨¡å‹é…ç½®

4. **æµ‹è¯•è„šæœ¬**ï¼š`eval_llm.py`
   - å¦‚ä½•åŠ è½½å’Œä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

---

## ğŸ¯ åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šä¿®æ”¹ eps å€¼

åœ¨ `exp2_norm_comparison.py` ä¸­ï¼Œå°† `eps` ä» `1e-5` æ”¹ä¸º `1e-8`ï¼Œè§‚å¯Ÿåœ¨ FP16 æ¨¡å¼ä¸‹æ˜¯å¦ä¼šå‡ºç°æ•°å€¼é—®é¢˜ã€‚

### ç»ƒä¹  2ï¼šå®ç° LayerNorm

å‚è€ƒ RMSNormï¼Œå®ç°ä¸€ä¸ª LayerNorm ç±»ï¼Œå¯¹æ¯”ä¸¤è€…çš„é€Ÿåº¦å·®å¼‚ã€‚

### ç»ƒä¹  3ï¼šå¯è§†åŒ–å½’ä¸€åŒ–æ•ˆæœ

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•æ¯ä¸€å±‚çš„æ¿€æ´»æ ‡å‡†å·®ï¼Œç»˜åˆ¶æ›²çº¿å›¾ï¼ŒéªŒè¯å½’ä¸€åŒ–æ˜¯å¦çœŸçš„ç¨³å®šäº†åˆ†å¸ƒã€‚

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- MiniMind å®Œæ•´ä»£ç ï¼š`model/model_minimind.py`
- Llama 2 æºç ï¼š[facebookresearch/llama](https://github.com/facebookresearch/llama)
- PyTorch LayerNorm æºç ï¼š[torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
