---
title: æ·±åº¦æŒæ¡ (30+å°æ—¶) | minimindä»é›¶ç†è§£llmè®­ç»ƒ
description: ä»é›¶è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„LLMã€‚æ·±å…¥æŒæ¡å¤§è¯­è¨€æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’Œä¼˜åŒ–æŠ€å·§ã€‚
keywords: LLMå®Œæ•´è®­ç»ƒ, å¤§æ¨¡å‹è®­ç»ƒ, ä»é›¶è®­ç»ƒLLM, Transformerå®Œæ•´æ•™ç¨‹, LLMæ·±åº¦æŒæ¡
---

# ğŸ“ æ·±åº¦æŒæ¡ (30+å°æ—¶)

> ä»é›¶è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„ LLM

## ğŸ¯ å­¦ä¹ ç›®æ ‡

30 å°æ—¶åä½ å°†èƒ½å¤Ÿ:
- âœ… ä»é›¶è®­ç»ƒä¸€ä¸ªå¯ç”¨çš„ LLM
- âœ… ç†è§£å®Œæ•´çš„è®­ç»ƒæµç¨‹
- âœ… è°ƒè¯•å’Œä¼˜åŒ–æ¨¡å‹
- âœ… åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹

## ğŸ“‹ å­¦ä¹ è·¯å¾„

### ç¬¬ 1 å‘¨: ç†è®ºåŸºç¡€ (6å°æ—¶)

âœ… å®Œæˆ [ğŸ“š ç³»ç»Ÿå­¦ä¹ ](./systematic)

---

### ç¬¬ 2 å‘¨: æ•°æ®å‡†å¤‡ (8å°æ—¶)

#### 1. Tokenizer è®­ç»ƒ (2å°æ—¶)

```bash
python scripts/train_tokenizer.py
```

**å­¦ä¹ å†…å®¹**:
- ç†è§£ BPE ç®—æ³•
- è®­ç»ƒè‡ªå®šä¹‰ tokenizer

#### 2. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç† (4å°æ—¶)

- é˜…è¯» `dataset/lm_dataset.py`
- ç†è§£æ•°æ®æ‰“åŒ…ï¼ˆpackingï¼‰ç­–ç•¥
- åˆ›å»ºè‡ªå·±çš„æ•°æ®é›†

#### 3. æ•°æ®æ ¼å¼è½¬æ¢ (2å°æ—¶)

- Pretrain æ ¼å¼
- SFT æ ¼å¼
- DPO æ ¼å¼

**å®Œæˆæ ‡å‡†**:
- [ ] èƒ½è®­ç»ƒä¸€ä¸ªè‡ªå®šä¹‰ tokenizer
- [ ] ç†è§£ä¸åŒè®­ç»ƒé˜¶æ®µçš„æ•°æ®æ ¼å¼
- [ ] å‡†å¤‡å¥½è®­ç»ƒæ•°æ®

---

### ç¬¬ 3 å‘¨: æ¨¡å‹è®­ç»ƒ (10å°æ—¶)

#### 1. Pretraining (4å°æ—¶)

```bash
cd trainer
python train_pretrain.py \
    --data_path ../dataset/pretrain_hq.jsonl \
    --epochs 1 \
    --batch_size 32 \
    --hidden_size 512 \
    --num_hidden_layers 8
```

**å­¦ä¹ è¦ç‚¹**:
- ç†è§£ Causal Language Modeling ç›®æ ‡
- ç›‘æ§è®­ç»ƒæ›²çº¿ï¼ˆlossã€learning rateï¼‰
- è°ƒè¯•å¸¸è§é—®é¢˜ï¼ˆNaNã€OOMï¼‰

---

#### 2. Supervised Fine-Tuning (3å°æ—¶)

```bash
python train_full_sft.py \
    --data_path ../dataset/sft_mini_512.jsonl \
    --from_weight pretrain
```

**å­¦ä¹ è¦ç‚¹**:
- ç†è§£æŒ‡ä»¤å¾®è°ƒçš„ä½œç”¨
- å¯¹æ¯” pretrain vs SFT çš„æ•ˆæœ

---

#### 3. LoRA Fine-Tuning (3å°æ—¶)

```bash
python train_lora.py \
    --data_path ../dataset/lora_identity.jsonl \
    --from_weight full_sft
```

**å­¦ä¹ è¦ç‚¹**:
- ç†è§£å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰
- LoRA çš„æ•°å­¦åŸç†
- é¢†åŸŸé€‚é…ç­–ç•¥

**å®Œæˆæ ‡å‡†**:
- [ ] æˆåŠŸè®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ï¼ˆå›°æƒ‘åº¦ < 3.0ï¼‰
- [ ] ç†è§£ pretrain â†’ SFT â†’ LoRA çš„å®Œæ•´æµç¨‹
- [ ] èƒ½è°ƒè¯•è®­ç»ƒé—®é¢˜

---

### ç¬¬ 4 å‘¨: è¿›é˜¶ä¸»é¢˜ (6+å°æ—¶)

#### å¯é€‰æ–¹å‘ 1: RLHF / RLAIF (4å°æ—¶)

- DPOï¼ˆDirect Preference Optimizationï¼‰
- PPO/GRPOï¼ˆReinforcement Learningï¼‰

#### å¯é€‰æ–¹å‘ 2: æ¨ç†ä¼˜åŒ– (2å°æ—¶)

- KV Cache
- Flash Attention
- é‡åŒ–ï¼ˆINT8/INT4ï¼‰

#### å¯é€‰æ–¹å‘ 3: è¯„ä¼°å’Œåˆ†æ (2å°æ—¶)

- C-Eval / MMLU åŸºå‡†æµ‹è¯•
- é”™è¯¯åˆ†æ
- æ¶ˆèå®éªŒ

---

## ğŸ”— å‚è€ƒèµ„æº

- ğŸ“– [MiniMind åŸä»“åº“](https://github.com/jingyaogong/minimind)
- ğŸ“ [CLAUDE.md](/CLAUDE) - å®Œæ•´çš„å‘½ä»¤å‚è€ƒ
- ğŸ’» [è®­ç»ƒè„šæœ¬](/trainer/) - æ‰€æœ‰è®­ç»ƒä»£ç 

---

## ğŸ“ å­¦ä¹ å»ºè®®

### 1. å…ˆå®éªŒï¼Œå†ç†è®º

âŒ ä¸è¦å…ˆè¯»å®Œæ‰€æœ‰æ–‡æ¡£å†åŠ¨æ‰‹
âœ… å…ˆè·‘å®éªŒï¼Œå»ºç«‹ç›´è§‰ï¼Œå†çœ‹ç†è®º

### 2. å¯¹æ¯”å­¦ä¹ 

æ¯ä¸ªæ¨¡å—éƒ½é€šè¿‡å¯¹æ¯”å®éªŒå›ç­”:
- **ä¸è¿™æ ·åšä¼šæ€æ ·ï¼Ÿ**
- **å…¶ä»–æ–¹æ¡ˆä¸ºä»€ä¹ˆä¸è¡Œï¼Ÿ**

### 3. å¾ªåºæ¸è¿›

- **ç¬¬ä¸€é**ï¼šå¿«é€Ÿè¿‡ä¸€éï¼Œç†è§£å¤§æ¡†æ¶
- **ç¬¬äºŒé**ï¼šæ·±å…¥ç»†èŠ‚ï¼Œç†è§£æ•°å­¦åŸç†
- **ç¬¬ä¸‰é**ï¼šè‡ªå·±å®ç°ï¼Œå·©å›ºç†è§£

### 4. è®°å½•ç¬”è®°

åœ¨ [å­¦ä¹ æ—¥å¿—](/learning_log) è®°å½•ä½ çš„å­¦ä¹ è¿‡ç¨‹

---

## ğŸ¯ æ£€æŸ¥æ¸…å•

### ç†è®ºåŸºç¡€
- [ ] å®Œæˆç³»ç»Ÿå­¦ä¹  (6å°æ—¶)

### æ•°æ®å‡†å¤‡
- [ ] è®­ç»ƒè‡ªå®šä¹‰ tokenizer
- [ ] å‡†å¤‡è®­ç»ƒæ•°æ®
- [ ] ç†è§£æ•°æ®æ ¼å¼

### æ¨¡å‹è®­ç»ƒ
- [ ] å®Œæˆ Pretrain
- [ ] å®Œæˆ SFT
- [ ] å®Œæˆ LoRA

### è¿›é˜¶ä¸»é¢˜
- [ ] å°è¯• RLHF/RLAIF
- [ ] ä¼˜åŒ–æ¨ç†æ€§èƒ½
- [ ] è¯„ä¼°æ¨¡å‹æ•ˆæœ

---

å‡†å¤‡å¥½å¼€å§‹æ·±åº¦æŒæ¡ä¹‹æ—…äº†å—ï¼ŸğŸš€
